{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 8836: expected 4 fields, saw 5\n",
      "\n",
      "Skipping line 535882: expected 4 fields, saw 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./sentiment_analysis/Sentiment Analysis Dataset.csv',error_bad_lines=False)\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "y = np.zeros(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1219540</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Twitter keeps deleting my updatesss  And yes, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1492898</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>making plans for a lovely day tommorow  bubble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237195</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>That was the quickest work day of my life. And...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>174537</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>@dropdeadvictor  tï¿½ sim..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>769329</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>HEY GUYS EVERYONE ADD @MattWayneCeleb please a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ItemID  Sentiment SentimentSource  \\\n",
       "0  1219540          0    Sentiment140   \n",
       "1  1492898          1    Sentiment140   \n",
       "2  1237195          1    Sentiment140   \n",
       "3   174537          1    Sentiment140   \n",
       "4   769329          1    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0  Twitter keeps deleting my updatesss  And yes, ...  \n",
       "1  making plans for a lovely day tommorow  bubble...  \n",
       "2  That was the quickest work day of my life. And...  \n",
       "3                        @dropdeadvictor  tï¿½ sim..  \n",
       "4  HEY GUYS EVERYONE ADD @MattWayneCeleb please a...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(data, y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    157969\n",
       "1    157754\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    632423\n",
       "0    630466\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_valid.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    505950\n",
       "0    504361\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    126473\n",
       "0    126105\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('./sentiment_analysis/train_dataset.csv',index=False)\n",
    "X_test.to_csv('./sentiment_analysis/test_dataset.csv',index=False)\n",
    "X_valid.to_csv('./sentiment_analysis/valid_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_train,X_test, X_train_valid,y_train,y_test,y_train_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('sentiment_analysis/train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "import re\n",
    "import tqdm\n",
    "import json\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_word2vec(x):\n",
    "    sent = []\n",
    "    out = x.strip()\n",
    "    out= \"\".join(i for i in out if ord(i)<128)\n",
    "    out = sent_tokenize(out)\n",
    "    for i in out:\n",
    "        t = word_tokenize(re.sub(r\"[^a-z0-9]+\", \" \", i.lower()))\n",
    "        sent.append(t)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e600924bf8e64731a9514ec38f22fbe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for i in tqdm.tqdm_notebook(x_train.SentimentText):\n",
    "    sentences.append(prepare_data_word2vec(i))\n",
    "    #except UnicodeDecodeError :\n",
    "    #    print i.encode('utf-8')\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = [item for sublist in sentences for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1735107"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = word2vec.Word2Vec(sentences=sents, size=100, window=5,iter=10, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaurya/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lmao', 0.8368069529533386),\n",
       " ('haha', 0.8177106380462646),\n",
       " ('hahaha', 0.6593310832977295),\n",
       " ('lmfao', 0.6513242125511169),\n",
       " ('hehe', 0.6214161515235901),\n",
       " ('lolz', 0.6208990812301636),\n",
       " ('hahah', 0.6194578409194946),\n",
       " ('cuz', 0.6178313493728638),\n",
       " ('rofl', 0.603983461856842),\n",
       " ('lmaoo', 0.5976788401603699)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(\"lol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaurya/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#### Save weights\n",
    "weights = w2v_model.wv.syn0\n",
    "np.save(open('./sentiment_analysis/w2v_weights', 'wb'), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = dict([(k, v.index) for k, v in w2v_model.wv.vocab.items()])\n",
    "with open('./sentiment_analysis/w2v_vocab', 'w') as f:\n",
    "    f.write(json.dumps(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Save word2vec model\n",
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.save('./sentiment_analysis/w2v_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.load('./sentiment_analysis/w2v_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(vocab_path):\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    word2idx = data\n",
    "    idx2word = dict([(v, k) for k, v in data.items()])\n",
    "    return word2idx, idx2word\n",
    "\n",
    "word2id,id2word = load_vocab('./sentiment_analysis/w2v_vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(x):\n",
    "    max_len = 35\n",
    "    sent = []\n",
    "    out = x.strip()\n",
    "    out= \"\".join(i for i in out if ord(i)<128)\n",
    "    out = word_tokenize(re.sub(r\"[^a-z0-9]+\", \" \", out.lower()))\n",
    "    out_ = []\n",
    "    for i in out:\n",
    "        try:\n",
    "            out_.append(word2id[i])\n",
    "        except:\n",
    "            continue\n",
    "    padded = np.zeros(max_len, dtype=np.int64)\n",
    "    leng = len(out_)\n",
    "    if leng > max_len:\n",
    "        padded = out_[:max_len]\n",
    "    else:\n",
    "        #print out_\n",
    "        padded[(max_len-leng):]=out_\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset():\n",
    "    \n",
    "    def __init__(self,filepath):\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        self.texts = self.data.iloc[:,3]\n",
    "        self.labels = self.data.iloc[:,1]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        text = np.array(prepare_data(text))\n",
    "        label = np.array(label).reshape(-1)\n",
    "        return text, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(CustomDataset('./sentiment_analysis/train_dataset.csv'),\n",
    "                         batch_size=64,\n",
    "                         shuffle=False)\n",
    "valid_loader = DataLoader(CustomDataset('./sentiment_analysis/valid_dataset.csv'),\n",
    "                         batch_size=64,\n",
    "                         shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(CustomDataset('./sentiment_analysis/test_dataset.csv'),\n",
    "                         batch_size=64,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 35])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print i[0].shape\n",
    "    print i[1].shape\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loaders = {\"train\": train_loader, \"val\": valid_loader}\n",
    "data_lengths = {\"train\": train_loader.dataset.data.shape[0], \"val\": train_loader.dataset.data.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    #emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    #emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    weights_matrix = torch.FloatTensor(weights_matrix)\n",
    "    emb_layer = nn.Embedding.from_pretrained(weights_matrix)\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    else:\n",
    "        emb_layer.weight.requires_grad = True\n",
    "        \n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,weights_matrix,num_out):\n",
    "        super(LSTM,self).__init__()\n",
    "        # define all the components that will be used in the NN (these can be reused)\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, False)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=100,batch_first =True)\n",
    "        self.fc1 = nn.Linear(100,64)\n",
    "        self.fc2 = nn.Linear(64,num_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #print type(x)\n",
    "        #print x.shape\n",
    "        out = self.embedding(x)\n",
    "        #print out.shape\n",
    "        out,(hn, cn) = self.lstm(out)\n",
    "        out = self.fc1(out[:,-1,:])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (embedding): Embedding(57411, 100)\n",
      "  (lstm): LSTM(100, 100, batch_first=True)\n",
      "  (fc1): Linear(in_features=100, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(weights,1)\n",
    "print lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainable_params_(m):\n",
    "    return [p for p in m.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(lstm.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([57411, 100])\n",
      "lstm.weight_ih_l0 torch.Size([400, 100])\n",
      "lstm.weight_hh_l0 torch.Size([400, 100])\n",
      "lstm.bias_ih_l0 torch.Size([400])\n",
      "lstm.bias_hh_l0 torch.Size([400])\n",
      "fc1.weight torch.Size([64, 100])\n",
      "fc1.bias torch.Size([64])\n",
      "fc2.weight torch.Size([1, 64])\n",
      "fc2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in lstm.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print name, param.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, param in lstm.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if name == 'lstm.bias_ih_l0':\n",
    "            fc1_w = param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1_w.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-03 *\n",
       "       1.8560)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(fc1_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-02 *\n",
       "       5.9771)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(fc1_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    mean = torch.mean(var)\n",
    "    stddev = torch.std(var)\n",
    "    maximum = torch.max(var)\n",
    "    minimum = torch.min(var)\n",
    "    return mean, stddev, maximum, minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.00000e-03 *\n",
       "        1.8560), tensor(1.00000e-02 *\n",
       "        5.9771), tensor(1.00000e-02 *\n",
       "        9.9816), tensor(1.00000e-02 *\n",
       "        -9.9938))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_summaries(fc1_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'torch.Tensor'>, torch.Size([57411, 100]))\n",
      "(<class 'torch.Tensor'>, torch.Size([400, 100]))\n",
      "(<class 'torch.Tensor'>, torch.Size([400, 100]))\n",
      "(<class 'torch.Tensor'>, torch.Size([400]))\n",
      "(<class 'torch.Tensor'>, torch.Size([400]))\n",
      "(<class 'torch.Tensor'>, torch.Size([64, 100]))\n",
      "(<class 'torch.Tensor'>, torch.Size([64]))\n",
      "(<class 'torch.Tensor'>, torch.Size([1, 64]))\n",
      "(<class 'torch.Tensor'>, torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "for param in lstm.parameters():\n",
    "    print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object parameters at 0x7f6967d7b370>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "#optimizer = torch.optim.Adam(trainable_params_(lstm), lr=0.001)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm ./sentiment_analysis/tensorboard/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter('./sentiment_analysis/tensorboard')\n",
    "#writer2 = SummaryWriter('./sentiment_analysis/tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "Epoch [1/5], Step [100/15786], Loss: 0.5198\n",
      "Epoch [1/5], Step [200/15786], Loss: 0.2418\n",
      "Epoch [1/5], Step [300/15786], Loss: 0.3028\n",
      "Epoch [1/5], Step [400/15786], Loss: 0.3268\n",
      "Epoch [1/5], Step [500/15786], Loss: 0.2768\n",
      "Epoch [1/5], Step [600/15786], Loss: 0.3599\n",
      "Epoch [1/5], Step [700/15786], Loss: 0.2390\n",
      "Epoch [1/5], Step [800/15786], Loss: 0.2462\n",
      "Epoch [1/5], Step [900/15786], Loss: 0.2047\n",
      "Epoch [1/5], Step [1000/15786], Loss: 0.2508\n",
      "Epoch [1/5], Step [1100/15786], Loss: 0.2929\n",
      "Epoch [1/5], Step [1200/15786], Loss: 0.1269\n",
      "Epoch [1/5], Step [1300/15786], Loss: 0.2315\n",
      "Epoch [1/5], Step [1400/15786], Loss: 0.2264\n",
      "Epoch [1/5], Step [1500/15786], Loss: 0.2999\n",
      "Epoch [1/5], Step [1600/15786], Loss: 0.2365\n",
      "Epoch [1/5], Step [1700/15786], Loss: 0.3318\n",
      "Epoch [1/5], Step [1800/15786], Loss: 0.3992\n",
      "Epoch [1/5], Step [1900/15786], Loss: 0.2566\n",
      "Epoch [1/5], Step [2000/15786], Loss: 0.3147\n",
      "Epoch [1/5], Step [2100/15786], Loss: 0.3276\n",
      "Epoch [1/5], Step [2200/15786], Loss: 0.2714\n",
      "Epoch [1/5], Step [2300/15786], Loss: 0.2808\n",
      "Epoch [1/5], Step [2400/15786], Loss: 0.4097\n",
      "Epoch [1/5], Step [2500/15786], Loss: 0.2931\n",
      "Epoch [1/5], Step [2600/15786], Loss: 0.2665\n",
      "Epoch [1/5], Step [2700/15786], Loss: 0.2489\n",
      "Epoch [1/5], Step [2800/15786], Loss: 0.3985\n",
      "Epoch [1/5], Step [2900/15786], Loss: 0.2671\n",
      "Epoch [1/5], Step [3000/15786], Loss: 0.3737\n",
      "Epoch [1/5], Step [3100/15786], Loss: 0.3629\n",
      "Epoch [1/5], Step [3200/15786], Loss: 0.4487\n",
      "Epoch [1/5], Step [3300/15786], Loss: 0.2987\n",
      "Epoch [1/5], Step [3400/15786], Loss: 0.3310\n",
      "Epoch [1/5], Step [3500/15786], Loss: 0.4011\n",
      "Epoch [1/5], Step [3600/15786], Loss: 0.2704\n",
      "Epoch [1/5], Step [3700/15786], Loss: 0.3825\n",
      "Epoch [1/5], Step [3800/15786], Loss: 0.2137\n",
      "Epoch [1/5], Step [3900/15786], Loss: 0.2536\n",
      "Epoch [1/5], Step [4000/15786], Loss: 0.3488\n",
      "Epoch [1/5], Step [4100/15786], Loss: 0.3218\n",
      "Epoch [1/5], Step [4200/15786], Loss: 0.2326\n",
      "Epoch [1/5], Step [4300/15786], Loss: 0.3010\n",
      "Epoch [1/5], Step [4400/15786], Loss: 0.4051\n",
      "Epoch [1/5], Step [4500/15786], Loss: 0.3333\n",
      "Epoch [1/5], Step [4600/15786], Loss: 0.4019\n",
      "Epoch [1/5], Step [4700/15786], Loss: 0.4042\n",
      "Epoch [1/5], Step [4800/15786], Loss: 0.3380\n",
      "Epoch [1/5], Step [4900/15786], Loss: 0.3711\n",
      "Epoch [1/5], Step [5000/15786], Loss: 0.3926\n",
      "Epoch [1/5], Step [5100/15786], Loss: 0.2765\n",
      "Epoch [1/5], Step [5200/15786], Loss: 0.3062\n",
      "Epoch [1/5], Step [5300/15786], Loss: 0.4185\n",
      "Epoch [1/5], Step [5400/15786], Loss: 0.4633\n",
      "Epoch [1/5], Step [5500/15786], Loss: 0.5545\n",
      "Epoch [1/5], Step [5600/15786], Loss: 0.3720\n",
      "Epoch [1/5], Step [5700/15786], Loss: 0.2797\n",
      "Epoch [1/5], Step [5800/15786], Loss: 0.3244\n",
      "Epoch [1/5], Step [5900/15786], Loss: 0.2897\n",
      "Epoch [1/5], Step [6000/15786], Loss: 0.3666\n",
      "Epoch [1/5], Step [6100/15786], Loss: 0.3687\n",
      "Epoch [1/5], Step [6200/15786], Loss: 0.3119\n",
      "Epoch [1/5], Step [6300/15786], Loss: 0.3913\n",
      "Epoch [1/5], Step [6400/15786], Loss: 0.4853\n",
      "Epoch [1/5], Step [6500/15786], Loss: 0.3087\n",
      "Epoch [1/5], Step [6600/15786], Loss: 0.2031\n",
      "Epoch [1/5], Step [6700/15786], Loss: 0.3859\n",
      "Epoch [1/5], Step [6800/15786], Loss: 0.3181\n",
      "Epoch [1/5], Step [6900/15786], Loss: 0.2886\n",
      "Epoch [1/5], Step [7000/15786], Loss: 0.3487\n",
      "Epoch [1/5], Step [7100/15786], Loss: 0.3952\n",
      "Epoch [1/5], Step [7200/15786], Loss: 0.2339\n",
      "Epoch [1/5], Step [7300/15786], Loss: 0.3986\n",
      "Epoch [1/5], Step [7400/15786], Loss: 0.2849\n",
      "Epoch [1/5], Step [7500/15786], Loss: 0.1924\n",
      "Epoch [1/5], Step [7600/15786], Loss: 0.3248\n",
      "Epoch [1/5], Step [7700/15786], Loss: 0.2494\n",
      "Epoch [1/5], Step [7800/15786], Loss: 0.3477\n",
      "Epoch [1/5], Step [7900/15786], Loss: 0.4358\n",
      "Epoch [1/5], Step [8000/15786], Loss: 0.4061\n",
      "Epoch [1/5], Step [8100/15786], Loss: 0.3347\n",
      "Epoch [1/5], Step [8200/15786], Loss: 0.3189\n",
      "Epoch [1/5], Step [8300/15786], Loss: 0.2446\n",
      "Epoch [1/5], Step [8400/15786], Loss: 0.3308\n",
      "Epoch [1/5], Step [8500/15786], Loss: 0.2241\n",
      "Epoch [1/5], Step [8600/15786], Loss: 0.1848\n",
      "Epoch [1/5], Step [8700/15786], Loss: 0.3006\n",
      "Epoch [1/5], Step [8800/15786], Loss: 0.3645\n",
      "Epoch [1/5], Step [8900/15786], Loss: 0.4087\n",
      "Epoch [1/5], Step [9000/15786], Loss: 0.3889\n",
      "Epoch [1/5], Step [9100/15786], Loss: 0.3369\n",
      "Epoch [1/5], Step [9200/15786], Loss: 0.4254\n",
      "Epoch [1/5], Step [9300/15786], Loss: 0.2061\n",
      "Epoch [1/5], Step [9400/15786], Loss: 0.3724\n",
      "Epoch [1/5], Step [9500/15786], Loss: 0.4904\n",
      "Epoch [1/5], Step [9600/15786], Loss: 0.3838\n",
      "Epoch [1/5], Step [9700/15786], Loss: 0.3077\n",
      "Epoch [1/5], Step [9800/15786], Loss: 0.3329\n",
      "Epoch [1/5], Step [9900/15786], Loss: 0.2197\n",
      "Epoch [1/5], Step [10000/15786], Loss: 0.2159\n",
      "Epoch [1/5], Step [10100/15786], Loss: 0.3440\n",
      "Epoch [1/5], Step [10200/15786], Loss: 0.2713\n",
      "Epoch [1/5], Step [10300/15786], Loss: 0.3117\n",
      "Epoch [1/5], Step [10400/15786], Loss: 0.3993\n",
      "Epoch [1/5], Step [10500/15786], Loss: 0.2859\n",
      "Epoch [1/5], Step [10600/15786], Loss: 0.3889\n",
      "Epoch [1/5], Step [10700/15786], Loss: 0.3485\n",
      "Epoch [1/5], Step [10800/15786], Loss: 0.4512\n",
      "Epoch [1/5], Step [10900/15786], Loss: 0.3263\n",
      "Epoch [1/5], Step [11000/15786], Loss: 0.2680\n",
      "Epoch [1/5], Step [11100/15786], Loss: 0.2055\n",
      "Epoch [1/5], Step [11200/15786], Loss: 0.2328\n",
      "Epoch [1/5], Step [11300/15786], Loss: 0.2423\n",
      "Epoch [1/5], Step [11400/15786], Loss: 0.3211\n",
      "Epoch [1/5], Step [11500/15786], Loss: 0.2210\n",
      "Epoch [1/5], Step [11600/15786], Loss: 0.3492\n",
      "Epoch [1/5], Step [11700/15786], Loss: 0.3136\n",
      "Epoch [1/5], Step [11800/15786], Loss: 0.4063\n",
      "Epoch [1/5], Step [11900/15786], Loss: 0.3481\n",
      "Epoch [1/5], Step [12000/15786], Loss: 0.3117\n",
      "Epoch [1/5], Step [12100/15786], Loss: 0.2810\n",
      "Epoch [1/5], Step [12200/15786], Loss: 0.3645\n",
      "Epoch [1/5], Step [12300/15786], Loss: 0.3639\n",
      "Epoch [1/5], Step [12400/15786], Loss: 0.3105\n",
      "Epoch [1/5], Step [12500/15786], Loss: 0.3162\n",
      "Epoch [1/5], Step [12600/15786], Loss: 0.4500\n",
      "Epoch [1/5], Step [12700/15786], Loss: 0.3103\n",
      "Epoch [1/5], Step [12800/15786], Loss: 0.4456\n",
      "Epoch [1/5], Step [12900/15786], Loss: 0.3694\n",
      "Epoch [1/5], Step [13000/15786], Loss: 0.2740\n",
      "Epoch [1/5], Step [13100/15786], Loss: 0.2699\n",
      "Epoch [1/5], Step [13200/15786], Loss: 0.3547\n",
      "Epoch [1/5], Step [13300/15786], Loss: 0.2762\n",
      "Epoch [1/5], Step [13400/15786], Loss: 0.3691\n",
      "Epoch [1/5], Step [13500/15786], Loss: 0.3633\n",
      "Epoch [1/5], Step [13600/15786], Loss: 0.2893\n",
      "Epoch [1/5], Step [13700/15786], Loss: 0.3181\n",
      "Epoch [1/5], Step [13800/15786], Loss: 0.2524\n",
      "Epoch [1/5], Step [13900/15786], Loss: 0.2348\n",
      "Epoch [1/5], Step [14000/15786], Loss: 0.3676\n",
      "Epoch [1/5], Step [14100/15786], Loss: 0.3206\n",
      "Epoch [1/5], Step [14200/15786], Loss: 0.4159\n",
      "Epoch [1/5], Step [14300/15786], Loss: 0.3954\n",
      "Epoch [1/5], Step [14400/15786], Loss: 0.4076\n",
      "Epoch [1/5], Step [14500/15786], Loss: 0.4272\n",
      "Epoch [1/5], Step [14600/15786], Loss: 0.2762\n",
      "Epoch [1/5], Step [14700/15786], Loss: 0.4110\n",
      "Epoch [1/5], Step [14800/15786], Loss: 0.3251\n",
      "Epoch [1/5], Step [14900/15786], Loss: 0.3796\n",
      "Epoch [1/5], Step [15000/15786], Loss: 0.4066\n",
      "Epoch [1/5], Step [15100/15786], Loss: 0.4372\n",
      "Epoch [1/5], Step [15200/15786], Loss: 0.3660\n",
      "Epoch [1/5], Step [15300/15786], Loss: 0.4292\n",
      "Epoch [1/5], Step [15400/15786], Loss: 0.3553\n",
      "Epoch [1/5], Step [15500/15786], Loss: 0.4295\n",
      "Epoch [1/5], Step [15600/15786], Loss: 0.3019\n",
      "Epoch [1/5], Step [15700/15786], Loss: 0.3954\n",
      "Epoch [1/5]train Loss: 0.3289\n",
      "Epoch [1/5]val Loss: 0.0952\n",
      "Epoch 1/4\n",
      "----------\n",
      "Epoch [2/5], Step [100/15786], Loss: 0.3578\n",
      "Epoch [2/5], Step [200/15786], Loss: 0.1391\n",
      "Epoch [2/5], Step [300/15786], Loss: 0.2196\n",
      "Epoch [2/5], Step [400/15786], Loss: 0.2525\n",
      "Epoch [2/5], Step [500/15786], Loss: 0.2246\n",
      "Epoch [2/5], Step [600/15786], Loss: 0.2172\n",
      "Epoch [2/5], Step [700/15786], Loss: 0.1347\n",
      "Epoch [2/5], Step [800/15786], Loss: 0.1950\n",
      "Epoch [2/5], Step [900/15786], Loss: 0.1558\n",
      "Epoch [2/5], Step [1000/15786], Loss: 0.1722\n",
      "Epoch [2/5], Step [1100/15786], Loss: 0.2222\n",
      "Epoch [2/5], Step [1200/15786], Loss: 0.1074\n",
      "Epoch [2/5], Step [1300/15786], Loss: 0.1526\n",
      "Epoch [2/5], Step [1400/15786], Loss: 0.1613\n",
      "Epoch [2/5], Step [1500/15786], Loss: 0.2819\n",
      "Epoch [2/5], Step [1600/15786], Loss: 0.2244\n",
      "Epoch [2/5], Step [1700/15786], Loss: 0.3161\n",
      "Epoch [2/5], Step [1800/15786], Loss: 0.3829\n",
      "Epoch [2/5], Step [1900/15786], Loss: 0.1677\n",
      "Epoch [2/5], Step [2000/15786], Loss: 0.2339\n",
      "Epoch [2/5], Step [2100/15786], Loss: 0.3277\n",
      "Epoch [2/5], Step [2200/15786], Loss: 0.1977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [2300/15786], Loss: 0.2619\n",
      "Epoch [2/5], Step [2400/15786], Loss: 0.2927\n",
      "Epoch [2/5], Step [2500/15786], Loss: 0.2610\n",
      "Epoch [2/5], Step [2600/15786], Loss: 0.2182\n",
      "Epoch [2/5], Step [2700/15786], Loss: 0.2429\n",
      "Epoch [2/5], Step [2800/15786], Loss: 0.3634\n",
      "Epoch [2/5], Step [2900/15786], Loss: 0.2506\n",
      "Epoch [2/5], Step [3000/15786], Loss: 0.3311\n",
      "Epoch [2/5], Step [3100/15786], Loss: 0.3164\n",
      "Epoch [2/5], Step [3200/15786], Loss: 0.4507\n",
      "Epoch [2/5], Step [3300/15786], Loss: 0.2713\n",
      "Epoch [2/5], Step [3400/15786], Loss: 0.3180\n",
      "Epoch [2/5], Step [3500/15786], Loss: 0.3495\n",
      "Epoch [2/5], Step [3600/15786], Loss: 0.2615\n",
      "Epoch [2/5], Step [3700/15786], Loss: 0.3603\n",
      "Epoch [2/5], Step [3800/15786], Loss: 0.2140\n",
      "Epoch [2/5], Step [3900/15786], Loss: 0.1994\n",
      "Epoch [2/5], Step [4000/15786], Loss: 0.3221\n",
      "Epoch [2/5], Step [4100/15786], Loss: 0.2350\n",
      "Epoch [2/5], Step [4200/15786], Loss: 0.2107\n",
      "Epoch [2/5], Step [4300/15786], Loss: 0.3136\n",
      "Epoch [2/5], Step [4400/15786], Loss: 0.3863\n",
      "Epoch [2/5], Step [4500/15786], Loss: 0.2692\n",
      "Epoch [2/5], Step [4600/15786], Loss: 0.2939\n",
      "Epoch [2/5], Step [4700/15786], Loss: 0.3470\n",
      "Epoch [2/5], Step [4800/15786], Loss: 0.2808\n",
      "Epoch [2/5], Step [4900/15786], Loss: 0.3086\n",
      "Epoch [2/5], Step [5000/15786], Loss: 0.3515\n",
      "Epoch [2/5], Step [5100/15786], Loss: 0.2525\n",
      "Epoch [2/5], Step [5200/15786], Loss: 0.2462\n",
      "Epoch [2/5], Step [5300/15786], Loss: 0.4336\n",
      "Epoch [2/5], Step [5400/15786], Loss: 0.3908\n",
      "Epoch [2/5], Step [5500/15786], Loss: 0.5454\n",
      "Epoch [2/5], Step [5600/15786], Loss: 0.2921\n",
      "Epoch [2/5], Step [5700/15786], Loss: 0.2199\n",
      "Epoch [2/5], Step [5800/15786], Loss: 0.2522\n",
      "Epoch [2/5], Step [5900/15786], Loss: 0.2741\n",
      "Epoch [2/5], Step [6000/15786], Loss: 0.3556\n",
      "Epoch [2/5], Step [6100/15786], Loss: 0.3348\n",
      "Epoch [2/5], Step [6200/15786], Loss: 0.2320\n",
      "Epoch [2/5], Step [6300/15786], Loss: 0.3628\n",
      "Epoch [2/5], Step [6400/15786], Loss: 0.4733\n",
      "Epoch [2/5], Step [6500/15786], Loss: 0.2511\n",
      "Epoch [2/5], Step [6600/15786], Loss: 0.1927\n",
      "Epoch [2/5], Step [6700/15786], Loss: 0.2917\n",
      "Epoch [2/5], Step [6800/15786], Loss: 0.2913\n",
      "Epoch [2/5], Step [6900/15786], Loss: 0.2500\n",
      "Epoch [2/5], Step [7000/15786], Loss: 0.3249\n",
      "Epoch [2/5], Step [7100/15786], Loss: 0.4385\n",
      "Epoch [2/5], Step [7200/15786], Loss: 0.1912\n",
      "Epoch [2/5], Step [7300/15786], Loss: 0.3186\n",
      "Epoch [2/5], Step [7400/15786], Loss: 0.2509\n",
      "Epoch [2/5], Step [7500/15786], Loss: 0.1563\n",
      "Epoch [2/5], Step [7600/15786], Loss: 0.2259\n",
      "Epoch [2/5], Step [7700/15786], Loss: 0.2334\n",
      "Epoch [2/5], Step [7800/15786], Loss: 0.3058\n",
      "Epoch [2/5], Step [7900/15786], Loss: 0.3489\n",
      "Epoch [2/5], Step [8000/15786], Loss: 0.3895\n",
      "Epoch [2/5], Step [8100/15786], Loss: 0.3159\n",
      "Epoch [2/5], Step [8200/15786], Loss: 0.2408\n",
      "Epoch [2/5], Step [8300/15786], Loss: 0.2048\n",
      "Epoch [2/5], Step [8400/15786], Loss: 0.2748\n",
      "Epoch [2/5], Step [8500/15786], Loss: 0.2104\n",
      "Epoch [2/5], Step [8600/15786], Loss: 0.1637\n",
      "Epoch [2/5], Step [8700/15786], Loss: 0.2416\n",
      "Epoch [2/5], Step [8800/15786], Loss: 0.3580\n",
      "Epoch [2/5], Step [8900/15786], Loss: 0.3530\n",
      "Epoch [2/5], Step [9000/15786], Loss: 0.3698\n",
      "Epoch [2/5], Step [9100/15786], Loss: 0.3142\n",
      "Epoch [2/5], Step [9200/15786], Loss: 0.3918\n",
      "Epoch [2/5], Step [9300/15786], Loss: 0.1550\n",
      "Epoch [2/5], Step [9400/15786], Loss: 0.2830\n",
      "Epoch [2/5], Step [9500/15786], Loss: 0.4423\n",
      "Epoch [2/5], Step [9600/15786], Loss: 0.3567\n",
      "Epoch [2/5], Step [9700/15786], Loss: 0.2217\n",
      "Epoch [2/5], Step [9800/15786], Loss: 0.2766\n",
      "Epoch [2/5], Step [9900/15786], Loss: 0.2020\n",
      "Epoch [2/5], Step [10000/15786], Loss: 0.1676\n",
      "Epoch [2/5], Step [10100/15786], Loss: 0.3210\n",
      "Epoch [2/5], Step [10200/15786], Loss: 0.2475\n",
      "Epoch [2/5], Step [10300/15786], Loss: 0.2190\n",
      "Epoch [2/5], Step [10400/15786], Loss: 0.3285\n",
      "Epoch [2/5], Step [10500/15786], Loss: 0.2626\n",
      "Epoch [2/5], Step [10600/15786], Loss: 0.3468\n",
      "Epoch [2/5], Step [10700/15786], Loss: 0.3378\n",
      "Epoch [2/5], Step [10800/15786], Loss: 0.4190\n",
      "Epoch [2/5], Step [10900/15786], Loss: 0.3071\n",
      "Epoch [2/5], Step [11000/15786], Loss: 0.2340\n",
      "Epoch [2/5], Step [11100/15786], Loss: 0.1505\n",
      "Epoch [2/5], Step [11200/15786], Loss: 0.2273\n",
      "Epoch [2/5], Step [11300/15786], Loss: 0.2280\n",
      "Epoch [2/5], Step [11400/15786], Loss: 0.2451\n",
      "Epoch [2/5], Step [11500/15786], Loss: 0.2011\n",
      "Epoch [2/5], Step [11600/15786], Loss: 0.2993\n",
      "Epoch [2/5], Step [11700/15786], Loss: 0.2696\n",
      "Epoch [2/5], Step [11800/15786], Loss: 0.4126\n",
      "Epoch [2/5], Step [11900/15786], Loss: 0.3291\n",
      "Epoch [2/5], Step [12000/15786], Loss: 0.2557\n",
      "Epoch [2/5], Step [12100/15786], Loss: 0.2725\n",
      "Epoch [2/5], Step [12200/15786], Loss: 0.3485\n",
      "Epoch [2/5], Step [12300/15786], Loss: 0.3794\n",
      "Epoch [2/5], Step [12400/15786], Loss: 0.2651\n",
      "Epoch [2/5], Step [12500/15786], Loss: 0.2750\n",
      "Epoch [2/5], Step [12600/15786], Loss: 0.3818\n",
      "Epoch [2/5], Step [12700/15786], Loss: 0.2531\n",
      "Epoch [2/5], Step [12800/15786], Loss: 0.4082\n",
      "Epoch [2/5], Step [12900/15786], Loss: 0.2909\n",
      "Epoch [2/5], Step [13000/15786], Loss: 0.2214\n",
      "Epoch [2/5], Step [13100/15786], Loss: 0.1967\n",
      "Epoch [2/5], Step [13200/15786], Loss: 0.2480\n",
      "Epoch [2/5], Step [13300/15786], Loss: 0.2235\n",
      "Epoch [2/5], Step [13400/15786], Loss: 0.3396\n",
      "Epoch [2/5], Step [13500/15786], Loss: 0.3491\n",
      "Epoch [2/5], Step [13600/15786], Loss: 0.2311\n",
      "Epoch [2/5], Step [13700/15786], Loss: 0.2682\n",
      "Epoch [2/5], Step [13800/15786], Loss: 0.2372\n",
      "Epoch [2/5], Step [13900/15786], Loss: 0.1970\n",
      "Epoch [2/5], Step [14000/15786], Loss: 0.3932\n",
      "Epoch [2/5], Step [14100/15786], Loss: 0.3117\n",
      "Epoch [2/5], Step [14200/15786], Loss: 0.3244\n",
      "Epoch [2/5], Step [14300/15786], Loss: 0.4009\n",
      "Epoch [2/5], Step [14400/15786], Loss: 0.3817\n",
      "Epoch [2/5], Step [14500/15786], Loss: 0.3935\n",
      "Epoch [2/5], Step [14600/15786], Loss: 0.2291\n",
      "Epoch [2/5], Step [14700/15786], Loss: 0.3805\n",
      "Epoch [2/5], Step [14800/15786], Loss: 0.2758\n",
      "Epoch [2/5], Step [14900/15786], Loss: 0.3201\n",
      "Epoch [2/5], Step [15000/15786], Loss: 0.3567\n",
      "Epoch [2/5], Step [15100/15786], Loss: 0.3785\n",
      "Epoch [2/5], Step [15200/15786], Loss: 0.2936\n",
      "Epoch [2/5], Step [15300/15786], Loss: 0.3955\n",
      "Epoch [2/5], Step [15400/15786], Loss: 0.3370\n",
      "Epoch [2/5], Step [15500/15786], Loss: 0.4034\n",
      "Epoch [2/5], Step [15600/15786], Loss: 0.2447\n",
      "Epoch [2/5], Step [15700/15786], Loss: 0.3526\n",
      "Epoch [2/5]train Loss: 0.2836\n",
      "Epoch [2/5]val Loss: 0.0990\n",
      "Epoch 2/4\n",
      "----------\n",
      "Epoch [3/5], Step [100/15786], Loss: 0.2948\n",
      "Epoch [3/5], Step [200/15786], Loss: 0.1334\n",
      "Epoch [3/5], Step [300/15786], Loss: 0.1708\n",
      "Epoch [3/5], Step [400/15786], Loss: 0.2147\n",
      "Epoch [3/5], Step [500/15786], Loss: 0.2004\n",
      "Epoch [3/5], Step [600/15786], Loss: 0.1903\n",
      "Epoch [3/5], Step [700/15786], Loss: 0.1147\n",
      "Epoch [3/5], Step [800/15786], Loss: 0.1463\n",
      "Epoch [3/5], Step [900/15786], Loss: 0.1210\n",
      "Epoch [3/5], Step [1000/15786], Loss: 0.1756\n",
      "Epoch [3/5], Step [1100/15786], Loss: 0.2141\n",
      "Epoch [3/5], Step [1200/15786], Loss: 0.0882\n",
      "Epoch [3/5], Step [1300/15786], Loss: 0.1422\n",
      "Epoch [3/5], Step [1400/15786], Loss: 0.2453\n",
      "Epoch [3/5], Step [1500/15786], Loss: 0.2935\n",
      "Epoch [3/5], Step [1600/15786], Loss: 0.2089\n",
      "Epoch [3/5], Step [1700/15786], Loss: 0.2290\n",
      "Epoch [3/5], Step [1800/15786], Loss: 0.4086\n",
      "Epoch [3/5], Step [1900/15786], Loss: 0.1711\n",
      "Epoch [3/5], Step [2000/15786], Loss: 0.2027\n",
      "Epoch [3/5], Step [2100/15786], Loss: 0.2886\n",
      "Epoch [3/5], Step [2200/15786], Loss: 0.1648\n",
      "Epoch [3/5], Step [2300/15786], Loss: 0.2062\n",
      "Epoch [3/5], Step [2400/15786], Loss: 0.2676\n",
      "Epoch [3/5], Step [2500/15786], Loss: 0.2486\n",
      "Epoch [3/5], Step [2600/15786], Loss: 0.1978\n",
      "Epoch [3/5], Step [2700/15786], Loss: 0.2132\n",
      "Epoch [3/5], Step [2800/15786], Loss: 0.3350\n",
      "Epoch [3/5], Step [2900/15786], Loss: 0.2065\n",
      "Epoch [3/5], Step [3000/15786], Loss: 0.3287\n",
      "Epoch [3/5], Step [3100/15786], Loss: 0.2911\n",
      "Epoch [3/5], Step [3200/15786], Loss: 0.4389\n",
      "Epoch [3/5], Step [3300/15786], Loss: 0.2551\n",
      "Epoch [3/5], Step [3400/15786], Loss: 0.2339\n",
      "Epoch [3/5], Step [3500/15786], Loss: 0.3441\n",
      "Epoch [3/5], Step [3600/15786], Loss: 0.1969\n",
      "Epoch [3/5], Step [3700/15786], Loss: 0.3397\n",
      "Epoch [3/5], Step [3800/15786], Loss: 0.1945\n",
      "Epoch [3/5], Step [3900/15786], Loss: 0.1667\n",
      "Epoch [3/5], Step [4000/15786], Loss: 0.2950\n",
      "Epoch [3/5], Step [4100/15786], Loss: 0.2111\n",
      "Epoch [3/5], Step [4200/15786], Loss: 0.1776\n",
      "Epoch [3/5], Step [4300/15786], Loss: 0.2604\n",
      "Epoch [3/5], Step [4400/15786], Loss: 0.3821\n",
      "Epoch [3/5], Step [4500/15786], Loss: 0.2582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [4600/15786], Loss: 0.2052\n",
      "Epoch [3/5], Step [4700/15786], Loss: 0.3359\n",
      "Epoch [3/5], Step [4800/15786], Loss: 0.2469\n",
      "Epoch [3/5], Step [4900/15786], Loss: 0.2582\n",
      "Epoch [3/5], Step [5000/15786], Loss: 0.2751\n",
      "Epoch [3/5], Step [5100/15786], Loss: 0.1897\n",
      "Epoch [3/5], Step [5200/15786], Loss: 0.2502\n",
      "Epoch [3/5], Step [5300/15786], Loss: 0.3465\n",
      "Epoch [3/5], Step [5400/15786], Loss: 0.3366\n",
      "Epoch [3/5], Step [5500/15786], Loss: 0.4795\n",
      "Epoch [3/5], Step [5600/15786], Loss: 0.2720\n",
      "Epoch [3/5], Step [5700/15786], Loss: 0.2044\n",
      "Epoch [3/5], Step [5800/15786], Loss: 0.1939\n",
      "Epoch [3/5], Step [5900/15786], Loss: 0.2500\n",
      "Epoch [3/5], Step [6000/15786], Loss: 0.3180\n",
      "Epoch [3/5], Step [6100/15786], Loss: 0.2692\n",
      "Epoch [3/5], Step [6200/15786], Loss: 0.1732\n",
      "Epoch [3/5], Step [6300/15786], Loss: 0.2901\n",
      "Epoch [3/5], Step [6400/15786], Loss: 0.4410\n",
      "Epoch [3/5], Step [6500/15786], Loss: 0.2406\n",
      "Epoch [3/5], Step [6600/15786], Loss: 0.1949\n",
      "Epoch [3/5], Step [6700/15786], Loss: 0.2428\n",
      "Epoch [3/5], Step [6800/15786], Loss: 0.2541\n",
      "Epoch [3/5], Step [6900/15786], Loss: 0.2287\n",
      "Epoch [3/5], Step [7000/15786], Loss: 0.3043\n",
      "Epoch [3/5], Step [7100/15786], Loss: 0.4209\n",
      "Epoch [3/5], Step [7200/15786], Loss: 0.1631\n",
      "Epoch [3/5], Step [7300/15786], Loss: 0.3049\n",
      "Epoch [3/5], Step [7400/15786], Loss: 0.2274\n",
      "Epoch [3/5], Step [7500/15786], Loss: 0.1443\n",
      "Epoch [3/5], Step [7600/15786], Loss: 0.1998\n",
      "Epoch [3/5], Step [7700/15786], Loss: 0.2178\n",
      "Epoch [3/5], Step [7800/15786], Loss: 0.2982\n",
      "Epoch [3/5], Step [7900/15786], Loss: 0.2792\n",
      "Epoch [3/5], Step [8000/15786], Loss: 0.3809\n",
      "Epoch [3/5], Step [8100/15786], Loss: 0.2950\n",
      "Epoch [3/5], Step [8200/15786], Loss: 0.1960\n",
      "Epoch [3/5], Step [8300/15786], Loss: 0.1801\n",
      "Epoch [3/5], Step [8400/15786], Loss: 0.2918\n",
      "Epoch [3/5], Step [8500/15786], Loss: 0.1989\n",
      "Epoch [3/5], Step [8600/15786], Loss: 0.1763\n",
      "Epoch [3/5], Step [8700/15786], Loss: 0.1983\n",
      "Epoch [3/5], Step [8800/15786], Loss: 0.3606\n",
      "Epoch [3/5], Step [8900/15786], Loss: 0.3247\n",
      "Epoch [3/5], Step [9000/15786], Loss: 0.3403\n",
      "Epoch [3/5], Step [9100/15786], Loss: 0.3034\n",
      "Epoch [3/5], Step [9200/15786], Loss: 0.4090\n",
      "Epoch [3/5], Step [9300/15786], Loss: 0.1686\n",
      "Epoch [3/5], Step [9400/15786], Loss: 0.2552\n",
      "Epoch [3/5], Step [9500/15786], Loss: 0.3792\n",
      "Epoch [3/5], Step [9600/15786], Loss: 0.3506\n",
      "Epoch [3/5], Step [9700/15786], Loss: 0.2222\n",
      "Epoch [3/5], Step [9800/15786], Loss: 0.2174\n",
      "Epoch [3/5], Step [9900/15786], Loss: 0.1756\n",
      "Epoch [3/5], Step [10000/15786], Loss: 0.1186\n",
      "Epoch [3/5], Step [10100/15786], Loss: 0.2930\n",
      "Epoch [3/5], Step [10200/15786], Loss: 0.1760\n",
      "Epoch [3/5], Step [10300/15786], Loss: 0.1829\n",
      "Epoch [3/5], Step [10400/15786], Loss: 0.2470\n",
      "Epoch [3/5], Step [10500/15786], Loss: 0.2301\n",
      "Epoch [3/5], Step [10600/15786], Loss: 0.3183\n",
      "Epoch [3/5], Step [10700/15786], Loss: 0.3119\n",
      "Epoch [3/5], Step [10800/15786], Loss: 0.3808\n",
      "Epoch [3/5], Step [10900/15786], Loss: 0.3023\n",
      "Epoch [3/5], Step [11000/15786], Loss: 0.2300\n",
      "Epoch [3/5], Step [11100/15786], Loss: 0.1422\n",
      "Epoch [3/5], Step [11200/15786], Loss: 0.1936\n",
      "Epoch [3/5], Step [11300/15786], Loss: 0.2044\n",
      "Epoch [3/5], Step [11400/15786], Loss: 0.2103\n",
      "Epoch [3/5], Step [11500/15786], Loss: 0.1613\n",
      "Epoch [3/5], Step [11600/15786], Loss: 0.2864\n",
      "Epoch [3/5], Step [11700/15786], Loss: 0.2503\n",
      "Epoch [3/5], Step [11800/15786], Loss: 0.3465\n",
      "Epoch [3/5], Step [11900/15786], Loss: 0.2778\n",
      "Epoch [3/5], Step [12000/15786], Loss: 0.2062\n",
      "Epoch [3/5], Step [12100/15786], Loss: 0.2339\n",
      "Epoch [3/5], Step [12200/15786], Loss: 0.2739\n",
      "Epoch [3/5], Step [12300/15786], Loss: 0.3810\n",
      "Epoch [3/5], Step [12400/15786], Loss: 0.2244\n",
      "Epoch [3/5], Step [12500/15786], Loss: 0.2604\n",
      "Epoch [3/5], Step [12600/15786], Loss: 0.3607\n",
      "Epoch [3/5], Step [12700/15786], Loss: 0.1916\n",
      "Epoch [3/5], Step [12800/15786], Loss: 0.3671\n",
      "Epoch [3/5], Step [12900/15786], Loss: 0.2267\n",
      "Epoch [3/5], Step [13000/15786], Loss: 0.2168\n",
      "Epoch [3/5], Step [13100/15786], Loss: 0.1619\n",
      "Epoch [3/5], Step [13200/15786], Loss: 0.2185\n",
      "Epoch [3/5], Step [13300/15786], Loss: 0.1741\n",
      "Epoch [3/5], Step [13400/15786], Loss: 0.2700\n",
      "Epoch [3/5], Step [13500/15786], Loss: 0.2891\n",
      "Epoch [3/5], Step [13600/15786], Loss: 0.2094\n",
      "Epoch [3/5], Step [13700/15786], Loss: 0.2421\n",
      "Epoch [3/5], Step [13800/15786], Loss: 0.2083\n",
      "Epoch [3/5], Step [13900/15786], Loss: 0.1660\n",
      "Epoch [3/5], Step [14000/15786], Loss: 0.3366\n",
      "Epoch [3/5], Step [14100/15786], Loss: 0.2568\n",
      "Epoch [3/5], Step [14200/15786], Loss: 0.2709\n",
      "Epoch [3/5], Step [14300/15786], Loss: 0.3642\n",
      "Epoch [3/5], Step [14400/15786], Loss: 0.3503\n",
      "Epoch [3/5], Step [14500/15786], Loss: 0.3436\n",
      "Epoch [3/5], Step [14600/15786], Loss: 0.2139\n",
      "Epoch [3/5], Step [14700/15786], Loss: 0.3102\n",
      "Epoch [3/5], Step [14800/15786], Loss: 0.2121\n",
      "Epoch [3/5], Step [14900/15786], Loss: 0.2137\n",
      "Epoch [3/5], Step [15000/15786], Loss: 0.3285\n",
      "Epoch [3/5], Step [15100/15786], Loss: 0.3393\n",
      "Epoch [3/5], Step [15200/15786], Loss: 0.2353\n",
      "Epoch [3/5], Step [15300/15786], Loss: 0.3486\n",
      "Epoch [3/5], Step [15400/15786], Loss: 0.2795\n",
      "Epoch [3/5], Step [15500/15786], Loss: 0.3769\n",
      "Epoch [3/5], Step [15600/15786], Loss: 0.1997\n",
      "Epoch [3/5], Step [15700/15786], Loss: 0.2667\n",
      "Epoch [3/5]train Loss: 0.2509\n",
      "Epoch [3/5]val Loss: 0.1097\n",
      "Epoch 3/4\n",
      "----------\n",
      "Epoch [4/5], Step [100/15786], Loss: 0.2899\n",
      "Epoch [4/5], Step [200/15786], Loss: 0.1064\n",
      "Epoch [4/5], Step [300/15786], Loss: 0.1515\n",
      "Epoch [4/5], Step [400/15786], Loss: 0.1846\n",
      "Epoch [4/5], Step [500/15786], Loss: 0.2006\n",
      "Epoch [4/5], Step [600/15786], Loss: 0.1625\n",
      "Epoch [4/5], Step [700/15786], Loss: 0.1209\n",
      "Epoch [4/5], Step [800/15786], Loss: 0.1481\n",
      "Epoch [4/5], Step [900/15786], Loss: 0.1092\n",
      "Epoch [4/5], Step [1000/15786], Loss: 0.1648\n",
      "Epoch [4/5], Step [1100/15786], Loss: 0.1854\n",
      "Epoch [4/5], Step [1200/15786], Loss: 0.0900\n",
      "Epoch [4/5], Step [1300/15786], Loss: 0.1483\n",
      "Epoch [4/5], Step [1400/15786], Loss: 0.1354\n",
      "Epoch [4/5], Step [1500/15786], Loss: 0.2476\n",
      "Epoch [4/5], Step [1600/15786], Loss: 0.1900\n",
      "Epoch [4/5], Step [1700/15786], Loss: 0.2124\n",
      "Epoch [4/5], Step [1800/15786], Loss: 0.4228\n",
      "Epoch [4/5], Step [1900/15786], Loss: 0.1577\n",
      "Epoch [4/5], Step [2000/15786], Loss: 0.1929\n",
      "Epoch [4/5], Step [2100/15786], Loss: 0.2640\n",
      "Epoch [4/5], Step [2200/15786], Loss: 0.1069\n",
      "Epoch [4/5], Step [2300/15786], Loss: 0.1720\n",
      "Epoch [4/5], Step [2400/15786], Loss: 0.2377\n",
      "Epoch [4/5], Step [2500/15786], Loss: 0.1936\n",
      "Epoch [4/5], Step [2600/15786], Loss: 0.2032\n",
      "Epoch [4/5], Step [2700/15786], Loss: 0.2115\n",
      "Epoch [4/5], Step [2800/15786], Loss: 0.3547\n",
      "Epoch [4/5], Step [2900/15786], Loss: 0.1607\n",
      "Epoch [4/5], Step [3000/15786], Loss: 0.2952\n",
      "Epoch [4/5], Step [3100/15786], Loss: 0.2677\n",
      "Epoch [4/5], Step [3200/15786], Loss: 0.3771\n",
      "Epoch [4/5], Step [3300/15786], Loss: 0.2268\n",
      "Epoch [4/5], Step [3400/15786], Loss: 0.2152\n",
      "Epoch [4/5], Step [3500/15786], Loss: 0.3241\n",
      "Epoch [4/5], Step [3600/15786], Loss: 0.1916\n",
      "Epoch [4/5], Step [3700/15786], Loss: 0.2704\n",
      "Epoch [4/5], Step [3800/15786], Loss: 0.2094\n",
      "Epoch [4/5], Step [3900/15786], Loss: 0.1571\n",
      "Epoch [4/5], Step [4000/15786], Loss: 0.2103\n",
      "Epoch [4/5], Step [4100/15786], Loss: 0.2007\n",
      "Epoch [4/5], Step [4200/15786], Loss: 0.1540\n",
      "Epoch [4/5], Step [4300/15786], Loss: 0.2314\n",
      "Epoch [4/5], Step [4400/15786], Loss: 0.3083\n",
      "Epoch [4/5], Step [4500/15786], Loss: 0.2405\n",
      "Epoch [4/5], Step [4600/15786], Loss: 0.2033\n",
      "Epoch [4/5], Step [4700/15786], Loss: 0.2527\n",
      "Epoch [4/5], Step [4800/15786], Loss: 0.2081\n",
      "Epoch [4/5], Step [4900/15786], Loss: 0.2289\n",
      "Epoch [4/5], Step [5000/15786], Loss: 0.2226\n",
      "Epoch [4/5], Step [5100/15786], Loss: 0.1741\n",
      "Epoch [4/5], Step [5200/15786], Loss: 0.1988\n",
      "Epoch [4/5], Step [5300/15786], Loss: 0.2794\n",
      "Epoch [4/5], Step [5400/15786], Loss: 0.3106\n",
      "Epoch [4/5], Step [5500/15786], Loss: 0.3620\n",
      "Epoch [4/5], Step [5600/15786], Loss: 0.2132\n",
      "Epoch [4/5], Step [5700/15786], Loss: 0.2159\n",
      "Epoch [4/5], Step [5800/15786], Loss: 0.1861\n",
      "Epoch [4/5], Step [5900/15786], Loss: 0.1943\n",
      "Epoch [4/5], Step [6000/15786], Loss: 0.3053\n",
      "Epoch [4/5], Step [6100/15786], Loss: 0.2383\n",
      "Epoch [4/5], Step [6200/15786], Loss: 0.1593\n",
      "Epoch [4/5], Step [6300/15786], Loss: 0.2808\n",
      "Epoch [4/5], Step [6400/15786], Loss: 0.3548\n",
      "Epoch [4/5], Step [6500/15786], Loss: 0.2415\n",
      "Epoch [4/5], Step [6600/15786], Loss: 0.1768\n",
      "Epoch [4/5], Step [6700/15786], Loss: 0.2686\n",
      "Epoch [4/5], Step [6800/15786], Loss: 0.2280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [6900/15786], Loss: 0.1952\n",
      "Epoch [4/5], Step [7000/15786], Loss: 0.2663\n",
      "Epoch [4/5], Step [7100/15786], Loss: 0.3083\n",
      "Epoch [4/5], Step [7200/15786], Loss: 0.1643\n",
      "Epoch [4/5], Step [7300/15786], Loss: 0.3052\n",
      "Epoch [4/5], Step [7400/15786], Loss: 0.1959\n",
      "Epoch [4/5], Step [7500/15786], Loss: 0.1156\n",
      "Epoch [4/5], Step [7600/15786], Loss: 0.1624\n",
      "Epoch [4/5], Step [7700/15786], Loss: 0.1954\n",
      "Epoch [4/5], Step [7800/15786], Loss: 0.2663\n",
      "Epoch [4/5], Step [7900/15786], Loss: 0.2692\n",
      "Epoch [4/5], Step [8000/15786], Loss: 0.2899\n",
      "Epoch [4/5], Step [8100/15786], Loss: 0.2858\n",
      "Epoch [4/5], Step [8200/15786], Loss: 0.2006\n",
      "Epoch [4/5], Step [8300/15786], Loss: 0.1971\n",
      "Epoch [4/5], Step [8400/15786], Loss: 0.2395\n",
      "Epoch [4/5], Step [8500/15786], Loss: 0.1865\n",
      "Epoch [4/5], Step [8600/15786], Loss: 0.1667\n",
      "Epoch [4/5], Step [8700/15786], Loss: 0.1825\n",
      "Epoch [4/5], Step [8800/15786], Loss: 0.3348\n",
      "Epoch [4/5], Step [8900/15786], Loss: 0.3402\n",
      "Epoch [4/5], Step [9000/15786], Loss: 0.3465\n",
      "Epoch [4/5], Step [9100/15786], Loss: 0.2341\n",
      "Epoch [4/5], Step [9200/15786], Loss: 0.4161\n",
      "Epoch [4/5], Step [9300/15786], Loss: 0.1008\n",
      "Epoch [4/5], Step [9400/15786], Loss: 0.2177\n",
      "Epoch [4/5], Step [9500/15786], Loss: 0.3279\n",
      "Epoch [4/5], Step [9600/15786], Loss: 0.3278\n",
      "Epoch [4/5], Step [9700/15786], Loss: 0.1931\n",
      "Epoch [4/5], Step [9800/15786], Loss: 0.1891\n",
      "Epoch [4/5], Step [9900/15786], Loss: 0.1850\n",
      "Epoch [4/5], Step [10000/15786], Loss: 0.0926\n",
      "Epoch [4/5], Step [10100/15786], Loss: 0.2705\n",
      "Epoch [4/5], Step [10200/15786], Loss: 0.1377\n",
      "Epoch [4/5], Step [10300/15786], Loss: 0.1515\n",
      "Epoch [4/5], Step [10400/15786], Loss: 0.2043\n",
      "Epoch [4/5], Step [10500/15786], Loss: 0.2101\n",
      "Epoch [4/5], Step [10600/15786], Loss: 0.2819\n",
      "Epoch [4/5], Step [10700/15786], Loss: 0.1984\n",
      "Epoch [4/5], Step [10800/15786], Loss: 0.3164\n",
      "Epoch [4/5], Step [10900/15786], Loss: 0.2375\n",
      "Epoch [4/5], Step [11000/15786], Loss: 0.2298\n",
      "Epoch [4/5], Step [11100/15786], Loss: 0.1246\n",
      "Epoch [4/5], Step [11200/15786], Loss: 0.1536\n",
      "Epoch [4/5], Step [11300/15786], Loss: 0.1983\n",
      "Epoch [4/5], Step [11400/15786], Loss: 0.1934\n",
      "Epoch [4/5], Step [11500/15786], Loss: 0.1341\n",
      "Epoch [4/5], Step [11600/15786], Loss: 0.2590\n",
      "Epoch [4/5], Step [11700/15786], Loss: 0.1772\n",
      "Epoch [4/5], Step [11800/15786], Loss: 0.3369\n",
      "Epoch [4/5], Step [11900/15786], Loss: 0.2808\n",
      "Epoch [4/5], Step [12000/15786], Loss: 0.2091\n",
      "Epoch [4/5], Step [12100/15786], Loss: 0.2518\n",
      "Epoch [4/5], Step [12200/15786], Loss: 0.2329\n",
      "Epoch [4/5], Step [12300/15786], Loss: 0.2402\n",
      "Epoch [4/5], Step [12400/15786], Loss: 0.1712\n",
      "Epoch [4/5], Step [12500/15786], Loss: 0.2218\n",
      "Epoch [4/5], Step [12600/15786], Loss: 0.3513\n",
      "Epoch [4/5], Step [12700/15786], Loss: 0.1665\n",
      "Epoch [4/5], Step [12800/15786], Loss: 0.3116\n",
      "Epoch [4/5], Step [12900/15786], Loss: 0.2204\n",
      "Epoch [4/5], Step [13000/15786], Loss: 0.1827\n",
      "Epoch [4/5], Step [13100/15786], Loss: 0.1386\n",
      "Epoch [4/5], Step [13200/15786], Loss: 0.1848\n",
      "Epoch [4/5], Step [13300/15786], Loss: 0.1597\n",
      "Epoch [4/5], Step [13400/15786], Loss: 0.2421\n",
      "Epoch [4/5], Step [13500/15786], Loss: 0.2636\n",
      "Epoch [4/5], Step [13600/15786], Loss: 0.1704\n",
      "Epoch [4/5], Step [13700/15786], Loss: 0.1800\n",
      "Epoch [4/5], Step [13800/15786], Loss: 0.1801\n",
      "Epoch [4/5], Step [13900/15786], Loss: 0.1318\n",
      "Epoch [4/5], Step [14000/15786], Loss: 0.3099\n",
      "Epoch [4/5], Step [14100/15786], Loss: 0.2018\n",
      "Epoch [4/5], Step [14200/15786], Loss: 0.2111\n",
      "Epoch [4/5], Step [14300/15786], Loss: 0.3321\n",
      "Epoch [4/5], Step [14400/15786], Loss: 0.2776\n",
      "Epoch [4/5], Step [14500/15786], Loss: 0.2827\n",
      "Epoch [4/5], Step [14600/15786], Loss: 0.2151\n",
      "Epoch [4/5], Step [14700/15786], Loss: 0.2936\n",
      "Epoch [4/5], Step [14800/15786], Loss: 0.1753\n",
      "Epoch [4/5], Step [14900/15786], Loss: 0.1684\n",
      "Epoch [4/5], Step [15000/15786], Loss: 0.3042\n",
      "Epoch [4/5], Step [15100/15786], Loss: 0.2794\n",
      "Epoch [4/5], Step [15200/15786], Loss: 0.1699\n",
      "Epoch [4/5], Step [15300/15786], Loss: 0.3084\n",
      "Epoch [4/5], Step [15400/15786], Loss: 0.2431\n",
      "Epoch [4/5], Step [15500/15786], Loss: 0.3281\n",
      "Epoch [4/5], Step [15600/15786], Loss: 0.1781\n",
      "Epoch [4/5], Step [15700/15786], Loss: 0.2470\n",
      "Epoch [4/5]train Loss: 0.2239\n",
      "Epoch [4/5]val Loss: 0.1268\n",
      "Epoch 4/4\n",
      "----------\n",
      "Epoch [5/5], Step [100/15786], Loss: 0.2673\n",
      "Epoch [5/5], Step [200/15786], Loss: 0.1211\n",
      "Epoch [5/5], Step [300/15786], Loss: 0.1357\n",
      "Epoch [5/5], Step [400/15786], Loss: 0.1530\n",
      "Epoch [5/5], Step [500/15786], Loss: 0.2017\n",
      "Epoch [5/5], Step [600/15786], Loss: 0.1753\n",
      "Epoch [5/5], Step [700/15786], Loss: 0.1381\n",
      "Epoch [5/5], Step [800/15786], Loss: 0.0759\n",
      "Epoch [5/5], Step [900/15786], Loss: 0.0902\n",
      "Epoch [5/5], Step [1000/15786], Loss: 0.1544\n",
      "Epoch [5/5], Step [1100/15786], Loss: 0.2204\n",
      "Epoch [5/5], Step [1200/15786], Loss: 0.0896\n",
      "Epoch [5/5], Step [1300/15786], Loss: 0.1315\n",
      "Epoch [5/5], Step [1400/15786], Loss: 0.1098\n",
      "Epoch [5/5], Step [1500/15786], Loss: 0.2080\n",
      "Epoch [5/5], Step [1600/15786], Loss: 0.1630\n",
      "Epoch [5/5], Step [1700/15786], Loss: 0.2105\n",
      "Epoch [5/5], Step [1800/15786], Loss: 0.4148\n",
      "Epoch [5/5], Step [1900/15786], Loss: 0.1495\n",
      "Epoch [5/5], Step [2000/15786], Loss: 0.2014\n",
      "Epoch [5/5], Step [2100/15786], Loss: 0.2332\n",
      "Epoch [5/5], Step [2200/15786], Loss: 0.1107\n",
      "Epoch [5/5], Step [2300/15786], Loss: 0.1534\n",
      "Epoch [5/5], Step [2400/15786], Loss: 0.2402\n",
      "Epoch [5/5], Step [2500/15786], Loss: 0.1336\n",
      "Epoch [5/5], Step [2600/15786], Loss: 0.1944\n",
      "Epoch [5/5], Step [2700/15786], Loss: 0.1839\n",
      "Epoch [5/5], Step [2800/15786], Loss: 0.3649\n",
      "Epoch [5/5], Step [2900/15786], Loss: 0.1583\n",
      "Epoch [5/5], Step [3000/15786], Loss: 0.2950\n",
      "Epoch [5/5], Step [3100/15786], Loss: 0.2737\n",
      "Epoch [5/5], Step [3200/15786], Loss: 0.3340\n",
      "Epoch [5/5], Step [3300/15786], Loss: 0.1834\n",
      "Epoch [5/5], Step [3400/15786], Loss: 0.1748\n",
      "Epoch [5/5], Step [3500/15786], Loss: 0.2650\n",
      "Epoch [5/5], Step [3600/15786], Loss: 0.2011\n",
      "Epoch [5/5], Step [3700/15786], Loss: 0.2366\n",
      "Epoch [5/5], Step [3800/15786], Loss: 0.1784\n",
      "Epoch [5/5], Step [3900/15786], Loss: 0.1485\n",
      "Epoch [5/5], Step [4000/15786], Loss: 0.2124\n",
      "Epoch [5/5], Step [4100/15786], Loss: 0.1968\n",
      "Epoch [5/5], Step [4200/15786], Loss: 0.1804\n",
      "Epoch [5/5], Step [4300/15786], Loss: 0.2682\n",
      "Epoch [5/5], Step [4400/15786], Loss: 0.2677\n",
      "Epoch [5/5], Step [4500/15786], Loss: 0.2639\n",
      "Epoch [5/5], Step [4600/15786], Loss: 0.1940\n",
      "Epoch [5/5], Step [4700/15786], Loss: 0.2920\n",
      "Epoch [5/5], Step [4800/15786], Loss: 0.2375\n",
      "Epoch [5/5], Step [4900/15786], Loss: 0.2013\n",
      "Epoch [5/5], Step [5000/15786], Loss: 0.1882\n",
      "Epoch [5/5], Step [5100/15786], Loss: 0.1376\n",
      "Epoch [5/5], Step [5200/15786], Loss: 0.1845\n",
      "Epoch [5/5], Step [5300/15786], Loss: 0.2315\n",
      "Epoch [5/5], Step [5400/15786], Loss: 0.2852\n",
      "Epoch [5/5], Step [5500/15786], Loss: 0.3182\n",
      "Epoch [5/5], Step [5600/15786], Loss: 0.2419\n",
      "Epoch [5/5], Step [5700/15786], Loss: 0.1634\n",
      "Epoch [5/5], Step [5800/15786], Loss: 0.2127\n",
      "Epoch [5/5], Step [5900/15786], Loss: 0.1666\n",
      "Epoch [5/5], Step [6000/15786], Loss: 0.2622\n",
      "Epoch [5/5], Step [6100/15786], Loss: 0.2433\n",
      "Epoch [5/5], Step [6200/15786], Loss: 0.1271\n",
      "Epoch [5/5], Step [6300/15786], Loss: 0.2505\n",
      "Epoch [5/5], Step [6400/15786], Loss: 0.2840\n",
      "Epoch [5/5], Step [6500/15786], Loss: 0.2015\n",
      "Epoch [5/5], Step [6600/15786], Loss: 0.1683\n",
      "Epoch [5/5], Step [6700/15786], Loss: 0.2237\n",
      "Epoch [5/5], Step [6800/15786], Loss: 0.2625\n",
      "Epoch [5/5], Step [6900/15786], Loss: 0.1463\n",
      "Epoch [5/5], Step [7000/15786], Loss: 0.1487\n",
      "Epoch [5/5], Step [7100/15786], Loss: 0.2627\n",
      "Epoch [5/5], Step [7200/15786], Loss: 0.2358\n",
      "Epoch [5/5], Step [7300/15786], Loss: 0.2418\n",
      "Epoch [5/5], Step [7400/15786], Loss: 0.1593\n",
      "Epoch [5/5], Step [7500/15786], Loss: 0.1017\n",
      "Epoch [5/5], Step [7600/15786], Loss: 0.1429\n",
      "Epoch [5/5], Step [7700/15786], Loss: 0.1661\n",
      "Epoch [5/5], Step [7800/15786], Loss: 0.2696\n",
      "Epoch [5/5], Step [7900/15786], Loss: 0.2484\n",
      "Epoch [5/5], Step [8000/15786], Loss: 0.2447\n",
      "Epoch [5/5], Step [8100/15786], Loss: 0.2585\n",
      "Epoch [5/5], Step [8200/15786], Loss: 0.1969\n",
      "Epoch [5/5], Step [8300/15786], Loss: 0.1771\n",
      "Epoch [5/5], Step [8400/15786], Loss: 0.2175\n",
      "Epoch [5/5], Step [8500/15786], Loss: 0.1702\n",
      "Epoch [5/5], Step [8600/15786], Loss: 0.1480\n",
      "Epoch [5/5], Step [8700/15786], Loss: 0.1254\n",
      "Epoch [5/5], Step [8800/15786], Loss: 0.2727\n",
      "Epoch [5/5], Step [8900/15786], Loss: 0.3002\n",
      "Epoch [5/5], Step [9000/15786], Loss: 0.2917\n",
      "Epoch [5/5], Step [9100/15786], Loss: 0.2357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [9200/15786], Loss: 0.3657\n",
      "Epoch [5/5], Step [9300/15786], Loss: 0.0821\n",
      "Epoch [5/5], Step [9400/15786], Loss: 0.2301\n",
      "Epoch [5/5], Step [9500/15786], Loss: 0.2694\n",
      "Epoch [5/5], Step [9600/15786], Loss: 0.3423\n",
      "Epoch [5/5], Step [9700/15786], Loss: 0.1726\n",
      "Epoch [5/5], Step [9800/15786], Loss: 0.1893\n",
      "Epoch [5/5], Step [9900/15786], Loss: 0.1726\n",
      "Epoch [5/5], Step [10000/15786], Loss: 0.0788\n",
      "Epoch [5/5], Step [10100/15786], Loss: 0.2667\n",
      "Epoch [5/5], Step [10200/15786], Loss: 0.1158\n",
      "Epoch [5/5], Step [10300/15786], Loss: 0.1373\n",
      "Epoch [5/5], Step [10400/15786], Loss: 0.1821\n",
      "Epoch [5/5], Step [10500/15786], Loss: 0.2050\n",
      "Epoch [5/5], Step [10600/15786], Loss: 0.2723\n",
      "Epoch [5/5], Step [10700/15786], Loss: 0.2395\n",
      "Epoch [5/5], Step [10800/15786], Loss: 0.2609\n",
      "Epoch [5/5], Step [10900/15786], Loss: 0.2562\n",
      "Epoch [5/5], Step [11000/15786], Loss: 0.1814\n",
      "Epoch [5/5], Step [11100/15786], Loss: 0.1248\n",
      "Epoch [5/5], Step [11200/15786], Loss: 0.1639\n",
      "Epoch [5/5], Step [11300/15786], Loss: 0.2008\n",
      "Epoch [5/5], Step [11400/15786], Loss: 0.1641\n",
      "Epoch [5/5], Step [11500/15786], Loss: 0.1150\n",
      "Epoch [5/5], Step [11600/15786], Loss: 0.2459\n",
      "Epoch [5/5], Step [11700/15786], Loss: 0.1797\n",
      "Epoch [5/5], Step [11800/15786], Loss: 0.2792\n",
      "Epoch [5/5], Step [11900/15786], Loss: 0.2265\n",
      "Epoch [5/5], Step [12000/15786], Loss: 0.1956\n",
      "Epoch [5/5], Step [12100/15786], Loss: 0.2047\n",
      "Epoch [5/5], Step [12200/15786], Loss: 0.2380\n",
      "Epoch [5/5], Step [12300/15786], Loss: 0.2048\n",
      "Epoch [5/5], Step [12400/15786], Loss: 0.1226\n",
      "Epoch [5/5], Step [12500/15786], Loss: 0.1751\n",
      "Epoch [5/5], Step [12600/15786], Loss: 0.2891\n",
      "Epoch [5/5], Step [12700/15786], Loss: 0.1487\n",
      "Epoch [5/5], Step [12800/15786], Loss: 0.2772\n",
      "Epoch [5/5], Step [12900/15786], Loss: 0.2167\n",
      "Epoch [5/5], Step [13000/15786], Loss: 0.1827\n",
      "Epoch [5/5], Step [13100/15786], Loss: 0.1291\n",
      "Epoch [5/5], Step [13200/15786], Loss: 0.1000\n",
      "Epoch [5/5], Step [13300/15786], Loss: 0.1503\n",
      "Epoch [5/5], Step [13400/15786], Loss: 0.2122\n",
      "Epoch [5/5], Step [13500/15786], Loss: 0.2455\n",
      "Epoch [5/5], Step [13600/15786], Loss: 0.1466\n",
      "Epoch [5/5], Step [13700/15786], Loss: 0.1488\n",
      "Epoch [5/5], Step [13800/15786], Loss: 0.1764\n",
      "Epoch [5/5], Step [13900/15786], Loss: 0.1147\n",
      "Epoch [5/5], Step [14000/15786], Loss: 0.3033\n",
      "Epoch [5/5], Step [14100/15786], Loss: 0.1777\n",
      "Epoch [5/5], Step [14200/15786], Loss: 0.1815\n",
      "Epoch [5/5], Step [14300/15786], Loss: 0.2601\n",
      "Epoch [5/5], Step [14400/15786], Loss: 0.2798\n",
      "Epoch [5/5], Step [14500/15786], Loss: 0.2192\n",
      "Epoch [5/5], Step [14600/15786], Loss: 0.1648\n",
      "Epoch [5/5], Step [14700/15786], Loss: 0.2808\n",
      "Epoch [5/5], Step [14800/15786], Loss: 0.1731\n",
      "Epoch [5/5], Step [14900/15786], Loss: 0.1580\n",
      "Epoch [5/5], Step [15000/15786], Loss: 0.2587\n",
      "Epoch [5/5], Step [15100/15786], Loss: 0.2391\n",
      "Epoch [5/5], Step [15200/15786], Loss: 0.1923\n",
      "Epoch [5/5], Step [15300/15786], Loss: 0.3344\n",
      "Epoch [5/5], Step [15400/15786], Loss: 0.2227\n",
      "Epoch [5/5], Step [15500/15786], Loss: 0.2966\n",
      "Epoch [5/5], Step [15600/15786], Loss: 0.1702\n",
      "Epoch [5/5], Step [15700/15786], Loss: 0.2149\n",
      "Epoch [5/5]train Loss: 0.2014\n",
      "Epoch [5/5]val Loss: 0.1453\n"
     ]
    }
   ],
   "source": [
    "num_epoch =5\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "steps = 1\n",
    "\n",
    "for epoch in range( num_epoch):\n",
    "    \n",
    "    print('Epoch {}/{}'.format(epoch, num_epoch- 1))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            lstm.train() # Set model to training mode\n",
    "        else:\n",
    "            lstm.eval() # Set model to evaluate mode\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        # Iterate over data.\n",
    "        \n",
    "        for i, (texts,labels) in enumerate(data_loaders[phase]):\n",
    "            # torch can only train on Variable, so convert them to Variable\n",
    "            texts = Variable(texts)\n",
    "            labels = Variable(labels)\n",
    "\n",
    "            outputs = lstm(texts) # Forward pass: compute the output class given a image\n",
    "            loss = criterion(outputs,labels.float()) # Compute the loss: difference between the output class and the pre-given label\n",
    "            \n",
    "            ##########Tensorboard#############################\n",
    "            ##################################################\n",
    "            if phase=='train':\n",
    "                writer.add_scalar(\"loss\",loss, steps)\n",
    "                for name, param in lstm.named_parameters():\n",
    "                    mean_,stddev_,max_,min_ = variable_summaries(param.data)\n",
    "                    writer.add_scalar(name+ \"_mean\",mean_, steps)\n",
    "                    writer.add_scalar(name+ \"_stddev\",stddev_, steps)\n",
    "                    writer.add_scalar(name+ \"_max\",max_, steps)\n",
    "                    writer.add_scalar(name+ \"_min\",min_, steps)\n",
    "                    #writer.add_histogram(name + \"_hist\",param.data.numpy(),steps)\n",
    "                steps+=1\n",
    "                \n",
    "            \n",
    "            ###################################################\n",
    "            ###################################################\n",
    "            optimizer.zero_grad() # clear gradients for next train\n",
    "            if phase == 'train':\n",
    "                loss.backward() # backpropagation, compute gradients\n",
    "                optimizer.step() # apply gradients  and update the weights of hidden nodes\n",
    "                \n",
    "            running_loss += loss.data * texts.size(0)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                if (i+1) % 100 == 0 :\n",
    "                    print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %(epoch+1, num_epoch, i+1, data_lengths[phase]//64, loss.data))\n",
    "                    \n",
    "        epoch_loss = running_loss / data_lengths[phase]\n",
    "        if phase =='train':\n",
    "            train_losses.append(epoch_loss)\n",
    "            writer.add_scalar(\"total_train_loss\",epoch_loss, epoch+1)\n",
    "        if phase =='val':\n",
    "            valid_losses.append(epoch_loss)\n",
    "            \n",
    "            #####################Tensorboard###################################\n",
    "            ###################################################################\n",
    "            writer.add_scalar(\"total/valid_loss\",epoch_loss, epoch+1)\n",
    "            for name, param in lstm.named_parameters():\n",
    "                    mean_,stddev_,max_,min_ = variable_summaries(param.data)\n",
    "                    writer.add_scalar(\"total/\"+name+ \"_mean\",mean_, epoch+1)\n",
    "                    writer.add_scalar(\"total/\"+name+ \"_stddev\",stddev_, epoch+1)\n",
    "                    writer.add_scalar(\"total/\"+name+ \"_max\",max_, epoch+1)\n",
    "                    writer.add_scalar(\"total/\"+name+ \"_min\",min_, epoch+1)\n",
    "                    writer.add_histogram(\"total/\"+name + \"_hist\",param.data.numpy(),epoch+1)\n",
    "        print('Epoch [{}/{}]{} Loss: {:.4f}'.format(epoch+1, num_epoch,phase, epoch_loss))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 315723 test texts: 80.0000 %\n"
     ]
    }
   ],
   "source": [
    "lstm.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for texts, labels in test_loader:\n",
    "    labels = labels.float()\n",
    "    texts = Variable(texts)\n",
    "    outputs = lstm(texts)\n",
    "    predicted = (outputs.data >0.5).float()\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 315723 test texts: %.4f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
