{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I hate how you cant even say black paint anymo...</td>\n",
       "      <td>5tz52q</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What's the difference between a Jew in Nazi Ge...</td>\n",
       "      <td>5tz4dd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body      id  score\n",
       "0  I hate how you cant even say black paint anymo...  5tz52q      1\n",
       "1  What's the difference between a Jew in Nazi Ge...  5tz4dd      0"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_jokes = pd.read_json('./joke_dataset/reddit_jokes.json')\n",
    "reddit_jokes['body'] = reddit_jokes['title'] + ' \\n ' + reddit_jokes['body']\n",
    "del reddit_jokes['title']\n",
    "reddit_jokes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'I hate how you cant even say black paint anymore \\n Now I have to say \"Leroy can you please paint the fence?\"'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_jokes['body'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jokes = reddit_jokes['body']\n",
    "# Remove empty texts\n",
    "jokes = jokes[~(jokes=='')]\n",
    "\n",
    "# Ensure no nulls and no empty\n",
    "assert sum(jokes=='')==0\n",
    "assert sum(jokes.isnull())==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "jokes = jokes.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a penny lies below the cliff where a jew and black man jumps from, who wins? \n",
      " we don't have enough information to conclude who's likely to win\n",
      "-----\n",
      "where did hitler hide his armies? \n",
      " in his sleevies!\n",
      "-----\n",
      "what would the name be of a magician duo containing a chicken and a deaf woman? \n",
      " hen and keller. \n",
      "-----\n",
      "what is hitler's least favorite month? \n",
      " jew-ly\n",
      "\n",
      "edit: how about jan-jew-ary, or jewne?\n",
      "-----\n",
      "why did the chicken cross the road? \n",
      " because that two-timing chicken head just couldn't resist that outside cock.  these chicks ain't loyal. the road trusted her to be faithful but in the end she just couldn't do it. that's ok though, because the road knows that karma is real. it will ensure that that chicken never crosses this road again.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    print(random.choice(list(jokes)))\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194553\n"
     ]
    }
   ],
   "source": [
    "print len(jokes)\n",
    "jokes = jokes[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print len(jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.zeros(len(jokes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_valid, X_test, y_train_valid, y_valid = train_test_split(jokes, y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.to_excel('./joke_dataset/train_dataset.xlsx',index=False)\n",
    "X_test.to_excel('./joke_dataset/test_dataset.xlsx',index=False)\n",
    "X_valid.to_excel('./joke_dataset/valid_dataset.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Create token_dict based on ascii letters\n",
    "all_letters = string.ascii_letters[0:26] + \"\"\" \".,;'-+=?!$%():\\n\"\"\"\n",
    "token_dict = {token:k for token,k in zip(all_letters, range(1,len(all_letters)+1))}\n",
    "\n",
    "end_token = '<end>'\n",
    "pad_token = '<pad>'\n",
    "\n",
    "token_dict[pad_token] = 0\n",
    "token_dict[end_token] = len(token_dict)\n",
    "\n",
    "decoder_dict = {token_dict[k]:k for k in token_dict.keys()}\n",
    "tokens_count = len(token_dict)\n",
    "assert len(decoder_dict)==len(decoder_dict)\n",
    "tokens_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_word = {}\n",
    "for i,j in token_dict.iteritems():\n",
    "    id_word[j]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_encoder(text):\n",
    "    return [token_dict[token] for token in text if token in token_dict] + [token_dict['<end>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 27, 1, 13, 44]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder(\"i am\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset():\n",
    "    \n",
    "    def __init__(self,filepath):\n",
    "        self.data = pd.read_excel(filepath)\n",
    "        self.texts = self.data.iloc[:,0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        data = self.texts[index]\n",
    "        encoded_data = text_encoder(data)\n",
    "        text = np.array(encoded_data[:-1])\n",
    "        label = np.array(encoded_data[1:])\n",
    "        return text, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    #target = torch.LongTensor(target)\n",
    "    return [data, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(CustomDataset('./joke_dataset/train_dataset.xlsx'),\n",
    "                         batch_size=10,\n",
    "                        shuffle=False,\n",
    "                         collate_fn=my_collate)\n",
    "valid_loader = DataLoader(CustomDataset('./joke_dataset/valid_dataset.xlsx'),\n",
    "                         batch_size=10,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=my_collate)\n",
    "\n",
    "test_loader = DataLoader(CustomDataset('./joke_dataset/test_dataset.xlsx'),\n",
    "                         batch_size=10,\n",
    "                         shuffle=False,collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "for i,j in valid_loader:\n",
    "    print len(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loaders = {\"train\": train_loader, \"val\": valid_loader}\n",
    "data_lengths = {\"train\": train_loader.dataset.data.shape[0], \"val\": train_loader.dataset.data.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,tokens_count,embedding_dimension):\n",
    "        super(LanguageModel,self).__init__()\n",
    "        self.embedding = nn.Embedding(tokens_count, embedding_dimension)\n",
    "        self.lstm = nn.LSTM(embedding_dimension,100,batch_first = True)\n",
    "        self.linear = nn.Linear(100,tokens_count)\n",
    "        \n",
    "    def forward(self,x,seq_length):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seq_length, batch_first=True)\n",
    "        packed_output, (ht, ct) = self.lstm(packed_embedded)\n",
    "        lstm_output, length = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        output = self.linear(lstm_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languagemodel = LanguageModel(tokens_count,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method LanguageModel.parameters of LanguageModel(\n",
       "  (embedding): Embedding(45, 100)\n",
       "  (lstm): LSTM(100, 100, batch_first=True)\n",
       "  (linear): Linear(in_features=100, out_features=45, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languagemodel.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(languagemodel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lengths(v):\n",
    "    return np.array([i for i in map(len, v)])\n",
    "\n",
    "def pad(v):\n",
    "    lens = np.array([len(item) for item in v])\n",
    "    mask = lens[:,None] > np.arange(lens.max())\n",
    "    out = np.zeros(mask.shape,dtype=int)\n",
    "    out[mask] = np.concatenate(v)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "Epoch [1/15], Step [5/320], Loss: 3.7224\n",
      "Epoch [1/15], Step [10/320], Loss: 3.6737\n",
      "Epoch [1/15], Step [15/320], Loss: 3.6117\n",
      "Epoch [1/15], Step [20/320], Loss: 3.3702\n",
      "Epoch [1/15], Step [25/320], Loss: 3.5598\n",
      "Epoch [1/15], Step [30/320], Loss: 3.5075\n",
      "Epoch [1/15], Step [35/320], Loss: 3.4607\n",
      "Epoch [1/15], Step [40/320], Loss: 3.4033\n",
      "Epoch [1/15], Step [45/320], Loss: 3.4864\n",
      "Epoch [1/15], Step [50/320], Loss: 3.5094\n",
      "Epoch [1/15], Step [55/320], Loss: 3.4248\n",
      "Epoch [1/15], Step [60/320], Loss: 3.3937\n",
      "Epoch [1/15], Step [65/320], Loss: 3.1433\n",
      "Epoch [1/15], Step [70/320], Loss: 2.8606\n",
      "Epoch [1/15], Step [75/320], Loss: 3.4746\n",
      "Epoch [1/15], Step [80/320], Loss: 3.0530\n",
      "Epoch [1/15], Step [85/320], Loss: 3.4350\n",
      "Epoch [1/15], Step [90/320], Loss: 3.3687\n",
      "Epoch [1/15], Step [95/320], Loss: 3.0790\n",
      "Epoch [1/15], Step [100/320], Loss: 3.1513\n",
      "Epoch [1/15], Step [105/320], Loss: 3.2930\n",
      "Epoch [1/15], Step [110/320], Loss: 3.3498\n",
      "Epoch [1/15], Step [115/320], Loss: 3.0772\n",
      "Epoch [1/15], Step [120/320], Loss: 3.3028\n",
      "Epoch [1/15], Step [125/320], Loss: 3.3166\n",
      "Epoch [1/15], Step [130/320], Loss: 3.1045\n",
      "Epoch [1/15], Step [135/320], Loss: 3.3097\n",
      "Epoch [1/15], Step [140/320], Loss: 3.1687\n",
      "Epoch [1/15], Step [145/320], Loss: 3.3151\n",
      "Epoch [1/15], Step [150/320], Loss: 3.2145\n",
      "Epoch [1/15], Step [155/320], Loss: 3.1177\n",
      "Epoch [1/15], Step [160/320], Loss: 3.0242\n",
      "Epoch [1/15], Step [165/320], Loss: 3.0075\n",
      "Epoch [1/15], Step [170/320], Loss: 3.2465\n",
      "Epoch [1/15], Step [175/320], Loss: 3.1256\n",
      "Epoch [1/15], Step [180/320], Loss: 3.1988\n",
      "Epoch [1/15], Step [185/320], Loss: 2.9584\n",
      "Epoch [1/15], Step [190/320], Loss: 3.1744\n",
      "Epoch [1/15], Step [195/320], Loss: 2.7638\n",
      "Epoch [1/15], Step [200/320], Loss: 3.0353\n",
      "Epoch [1/15], Step [205/320], Loss: 2.9169\n",
      "Epoch [1/15], Step [210/320], Loss: 3.0941\n",
      "Epoch [1/15], Step [215/320], Loss: 3.1347\n",
      "Epoch [1/15], Step [220/320], Loss: 3.1157\n",
      "Epoch [1/15], Step [225/320], Loss: 2.8966\n",
      "Epoch [1/15], Step [230/320], Loss: 2.9486\n",
      "Epoch [1/15], Step [235/320], Loss: 3.0569\n",
      "Epoch [1/15], Step [240/320], Loss: 3.1028\n",
      "Epoch [1/15], Step [245/320], Loss: 2.9782\n",
      "Epoch [1/15], Step [250/320], Loss: 3.0596\n",
      "Epoch [1/15], Step [255/320], Loss: 2.5100\n",
      "Epoch [1/15], Step [260/320], Loss: 2.9546\n",
      "Epoch [1/15], Step [265/320], Loss: 2.9820\n",
      "Epoch [1/15], Step [270/320], Loss: 2.6628\n",
      "Epoch [1/15], Step [275/320], Loss: 2.9227\n",
      "Epoch [1/15], Step [280/320], Loss: 2.8513\n",
      "Epoch [1/15], Step [285/320], Loss: 2.9523\n",
      "Epoch [1/15], Step [290/320], Loss: 2.9584\n",
      "Epoch [1/15], Step [295/320], Loss: 2.9763\n",
      "Epoch [1/15], Step [300/320], Loss: 2.8535\n",
      "Epoch [1/15], Step [305/320], Loss: 2.8283\n",
      "Epoch [1/15], Step [310/320], Loss: 2.9951\n",
      "Epoch [1/15], Step [315/320], Loss: 2.8024\n",
      "Epoch [1/15], Step [320/320], Loss: 2.9915\n",
      "Epoch [1/15]train Loss: 3.1472\n",
      "Epoch [1/15]val Loss: 0.7159\n",
      "Epoch 1/14\n",
      "----------\n",
      "Epoch [2/15], Step [5/320], Loss: 2.8843\n",
      "Epoch [2/15], Step [10/320], Loss: 2.8827\n",
      "Epoch [2/15], Step [15/320], Loss: 2.8895\n",
      "Epoch [2/15], Step [20/320], Loss: 2.6001\n",
      "Epoch [2/15], Step [25/320], Loss: 2.9229\n",
      "Epoch [2/15], Step [30/320], Loss: 2.8690\n",
      "Epoch [2/15], Step [35/320], Loss: 2.8390\n",
      "Epoch [2/15], Step [40/320], Loss: 2.7819\n",
      "Epoch [2/15], Step [45/320], Loss: 2.8780\n",
      "Epoch [2/15], Step [50/320], Loss: 2.9127\n",
      "Epoch [2/15], Step [55/320], Loss: 2.8369\n",
      "Epoch [2/15], Step [60/320], Loss: 2.8233\n",
      "Epoch [2/15], Step [65/320], Loss: 2.5981\n",
      "Epoch [2/15], Step [70/320], Loss: 2.3222\n",
      "Epoch [2/15], Step [75/320], Loss: 2.9016\n",
      "Epoch [2/15], Step [80/320], Loss: 2.5366\n",
      "Epoch [2/15], Step [85/320], Loss: 2.8669\n",
      "Epoch [2/15], Step [90/320], Loss: 2.8128\n",
      "Epoch [2/15], Step [95/320], Loss: 2.5769\n",
      "Epoch [2/15], Step [100/320], Loss: 2.6377\n",
      "Epoch [2/15], Step [105/320], Loss: 2.7461\n",
      "Epoch [2/15], Step [110/320], Loss: 2.7992\n",
      "Epoch [2/15], Step [115/320], Loss: 2.5802\n",
      "Epoch [2/15], Step [120/320], Loss: 2.7584\n",
      "Epoch [2/15], Step [125/320], Loss: 2.7662\n",
      "Epoch [2/15], Step [130/320], Loss: 2.6387\n",
      "Epoch [2/15], Step [135/320], Loss: 2.7655\n",
      "Epoch [2/15], Step [140/320], Loss: 2.6662\n",
      "Epoch [2/15], Step [145/320], Loss: 2.7632\n",
      "Epoch [2/15], Step [150/320], Loss: 2.6945\n",
      "Epoch [2/15], Step [155/320], Loss: 2.6266\n",
      "Epoch [2/15], Step [160/320], Loss: 2.5611\n",
      "Epoch [2/15], Step [165/320], Loss: 2.5314\n",
      "Epoch [2/15], Step [170/320], Loss: 2.7050\n",
      "Epoch [2/15], Step [175/320], Loss: 2.6184\n",
      "Epoch [2/15], Step [180/320], Loss: 2.6726\n",
      "Epoch [2/15], Step [185/320], Loss: 2.5246\n",
      "Epoch [2/15], Step [190/320], Loss: 2.6533\n",
      "Epoch [2/15], Step [195/320], Loss: 2.3597\n",
      "Epoch [2/15], Step [200/320], Loss: 2.5433\n",
      "Epoch [2/15], Step [205/320], Loss: 2.4889\n",
      "Epoch [2/15], Step [210/320], Loss: 2.5885\n",
      "Epoch [2/15], Step [215/320], Loss: 2.6183\n",
      "Epoch [2/15], Step [220/320], Loss: 2.6030\n",
      "Epoch [2/15], Step [225/320], Loss: 2.4449\n",
      "Epoch [2/15], Step [230/320], Loss: 2.4858\n",
      "Epoch [2/15], Step [235/320], Loss: 2.5587\n",
      "Epoch [2/15], Step [240/320], Loss: 2.5902\n",
      "Epoch [2/15], Step [245/320], Loss: 2.5034\n",
      "Epoch [2/15], Step [250/320], Loss: 2.5617\n",
      "Epoch [2/15], Step [255/320], Loss: 2.1886\n",
      "Epoch [2/15], Step [260/320], Loss: 2.4918\n",
      "Epoch [2/15], Step [265/320], Loss: 2.5027\n",
      "Epoch [2/15], Step [270/320], Loss: 2.3029\n",
      "Epoch [2/15], Step [275/320], Loss: 2.4599\n",
      "Epoch [2/15], Step [280/320], Loss: 2.4316\n",
      "Epoch [2/15], Step [285/320], Loss: 2.4644\n",
      "Epoch [2/15], Step [290/320], Loss: 2.4691\n",
      "Epoch [2/15], Step [295/320], Loss: 2.4856\n",
      "Epoch [2/15], Step [300/320], Loss: 2.4137\n",
      "Epoch [2/15], Step [305/320], Loss: 2.3799\n",
      "Epoch [2/15], Step [310/320], Loss: 2.4950\n",
      "Epoch [2/15], Step [315/320], Loss: 2.4146\n",
      "Epoch [2/15], Step [320/320], Loss: 2.4876\n",
      "Epoch [2/15]train Loss: 2.6208\n",
      "Epoch [2/15]val Loss: 0.6019\n",
      "Epoch 2/14\n",
      "----------\n",
      "Epoch [3/15], Step [5/320], Loss: 2.4138\n",
      "Epoch [3/15], Step [10/320], Loss: 2.4087\n",
      "Epoch [3/15], Step [15/320], Loss: 2.4125\n",
      "Epoch [3/15], Step [20/320], Loss: 2.2692\n",
      "Epoch [3/15], Step [25/320], Loss: 2.4253\n",
      "Epoch [3/15], Step [30/320], Loss: 2.4077\n",
      "Epoch [3/15], Step [35/320], Loss: 2.3822\n",
      "Epoch [3/15], Step [40/320], Loss: 2.3553\n",
      "Epoch [3/15], Step [45/320], Loss: 2.3936\n",
      "Epoch [3/15], Step [50/320], Loss: 2.4014\n",
      "Epoch [3/15], Step [55/320], Loss: 2.3616\n",
      "Epoch [3/15], Step [60/320], Loss: 2.3512\n",
      "Epoch [3/15], Step [65/320], Loss: 2.2339\n",
      "Epoch [3/15], Step [70/320], Loss: 2.0546\n",
      "Epoch [3/15], Step [75/320], Loss: 2.3776\n",
      "Epoch [3/15], Step [80/320], Loss: 2.1862\n",
      "Epoch [3/15], Step [85/320], Loss: 2.3568\n",
      "Epoch [3/15], Step [90/320], Loss: 2.3244\n",
      "Epoch [3/15], Step [95/320], Loss: 2.2138\n",
      "Epoch [3/15], Step [100/320], Loss: 2.2337\n",
      "Epoch [3/15], Step [105/320], Loss: 2.2717\n",
      "Epoch [3/15], Step [110/320], Loss: 2.3081\n",
      "Epoch [3/15], Step [115/320], Loss: 2.1813\n",
      "Epoch [3/15], Step [120/320], Loss: 2.2790\n",
      "Epoch [3/15], Step [125/320], Loss: 2.2738\n",
      "Epoch [3/15], Step [130/320], Loss: 2.2617\n",
      "Epoch [3/15], Step [135/320], Loss: 2.2801\n",
      "Epoch [3/15], Step [140/320], Loss: 2.2363\n",
      "Epoch [3/15], Step [145/320], Loss: 2.2594\n",
      "Epoch [3/15], Step [150/320], Loss: 2.2299\n",
      "Epoch [3/15], Step [155/320], Loss: 2.2050\n",
      "Epoch [3/15], Step [160/320], Loss: 2.1771\n",
      "Epoch [3/15], Step [165/320], Loss: 2.1415\n",
      "Epoch [3/15], Step [170/320], Loss: 2.2142\n",
      "Epoch [3/15], Step [175/320], Loss: 2.1759\n",
      "Epoch [3/15], Step [180/320], Loss: 2.1993\n",
      "Epoch [3/15], Step [185/320], Loss: 2.1705\n",
      "Epoch [3/15], Step [190/320], Loss: 2.1850\n",
      "Epoch [3/15], Step [195/320], Loss: 2.0627\n",
      "Epoch [3/15], Step [200/320], Loss: 2.1189\n",
      "Epoch [3/15], Step [205/320], Loss: 2.1218\n",
      "Epoch [3/15], Step [210/320], Loss: 2.1384\n",
      "Epoch [3/15], Step [215/320], Loss: 2.1514\n",
      "Epoch [3/15], Step [220/320], Loss: 2.1436\n",
      "Epoch [3/15], Step [225/320], Loss: 2.0638\n",
      "Epoch [3/15], Step [230/320], Loss: 2.0836\n",
      "Epoch [3/15], Step [235/320], Loss: 2.1110\n",
      "Epoch [3/15], Step [240/320], Loss: 2.1270\n",
      "Epoch [3/15], Step [245/320], Loss: 2.0851\n",
      "Epoch [3/15], Step [250/320], Loss: 2.1156\n",
      "Epoch [3/15], Step [255/320], Loss: 1.9457\n",
      "Epoch [3/15], Step [260/320], Loss: 2.0825\n",
      "Epoch [3/15], Step [265/320], Loss: 2.0755\n",
      "Epoch [3/15], Step [270/320], Loss: 2.0061\n",
      "Epoch [3/15], Step [275/320], Loss: 2.0526\n",
      "Epoch [3/15], Step [280/320], Loss: 2.0667\n",
      "Epoch [3/15], Step [285/320], Loss: 2.0279\n",
      "Epoch [3/15], Step [290/320], Loss: 2.0322\n",
      "Epoch [3/15], Step [295/320], Loss: 2.0456\n",
      "Epoch [3/15], Step [300/320], Loss: 2.0295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15], Step [305/320], Loss: 1.9877\n",
      "Epoch [3/15], Step [310/320], Loss: 2.0390\n",
      "Epoch [3/15], Step [315/320], Loss: 2.0764\n",
      "Epoch [3/15], Step [320/320], Loss: 2.0308\n",
      "Epoch [3/15]train Loss: 2.1952\n",
      "Epoch [3/15]val Loss: 0.5004\n",
      "Epoch 3/14\n",
      "----------\n",
      "Epoch [4/15], Step [5/320], Loss: 1.9920\n",
      "Epoch [4/15], Step [10/320], Loss: 1.9809\n",
      "Epoch [4/15], Step [15/320], Loss: 1.9838\n",
      "Epoch [4/15], Step [20/320], Loss: 2.0012\n",
      "Epoch [4/15], Step [25/320], Loss: 1.9743\n",
      "Epoch [4/15], Step [30/320], Loss: 1.9933\n",
      "Epoch [4/15], Step [35/320], Loss: 1.9727\n",
      "Epoch [4/15], Step [40/320], Loss: 1.9793\n",
      "Epoch [4/15], Step [45/320], Loss: 1.9563\n",
      "Epoch [4/15], Step [50/320], Loss: 1.9351\n",
      "Epoch [4/15], Step [55/320], Loss: 1.9354\n",
      "Epoch [4/15], Step [60/320], Loss: 1.9280\n",
      "Epoch [4/15], Step [65/320], Loss: 1.9178\n",
      "Epoch [4/15], Step [70/320], Loss: 1.8474\n",
      "Epoch [4/15], Step [75/320], Loss: 1.9058\n",
      "Epoch [4/15], Step [80/320], Loss: 1.8875\n",
      "Epoch [4/15], Step [85/320], Loss: 1.8949\n",
      "Epoch [4/15], Step [90/320], Loss: 1.8845\n",
      "Epoch [4/15], Step [95/320], Loss: 1.9030\n",
      "Epoch [4/15], Step [100/320], Loss: 1.8810\n",
      "Epoch [4/15], Step [105/320], Loss: 1.8487\n",
      "Epoch [4/15], Step [110/320], Loss: 1.8658\n",
      "Epoch [4/15], Step [115/320], Loss: 1.8337\n",
      "Epoch [4/15], Step [120/320], Loss: 1.8483\n",
      "Epoch [4/15], Step [125/320], Loss: 1.8326\n",
      "Epoch [4/15], Step [130/320], Loss: 1.9292\n",
      "Epoch [4/15], Step [135/320], Loss: 1.8436\n",
      "Epoch [4/15], Step [140/320], Loss: 1.8543\n",
      "Epoch [4/15], Step [145/320], Loss: 1.8061\n",
      "Epoch [4/15], Step [150/320], Loss: 1.8129\n",
      "Epoch [4/15], Step [155/320], Loss: 1.8352\n",
      "Epoch [4/15], Step [160/320], Loss: 1.8438\n",
      "Epoch [4/15], Step [165/320], Loss: 1.8062\n",
      "Epoch [4/15], Step [170/320], Loss: 1.7741\n",
      "Epoch [4/15], Step [175/320], Loss: 1.7835\n",
      "Epoch [4/15], Step [180/320], Loss: 1.7764\n",
      "Epoch [4/15], Step [185/320], Loss: 1.8601\n",
      "Epoch [4/15], Step [190/320], Loss: 1.7673\n",
      "Epoch [4/15], Step [195/320], Loss: 1.8112\n",
      "Epoch [4/15], Step [200/320], Loss: 1.7482\n",
      "Epoch [4/15], Step [205/320], Loss: 1.8035\n",
      "Epoch [4/15], Step [210/320], Loss: 1.7398\n",
      "Epoch [4/15], Step [215/320], Loss: 1.7351\n",
      "Epoch [4/15], Step [220/320], Loss: 1.7367\n",
      "Epoch [4/15], Step [225/320], Loss: 1.7332\n",
      "Epoch [4/15], Step [230/320], Loss: 1.7295\n",
      "Epoch [4/15], Step [235/320], Loss: 1.7151\n",
      "Epoch [4/15], Step [240/320], Loss: 1.7170\n",
      "Epoch [4/15], Step [245/320], Loss: 1.7183\n",
      "Epoch [4/15], Step [250/320], Loss: 1.7242\n",
      "Epoch [4/15], Step [255/320], Loss: 1.7471\n",
      "Epoch [4/15], Step [260/320], Loss: 1.7231\n",
      "Epoch [4/15], Step [265/320], Loss: 1.7019\n",
      "Epoch [4/15], Step [270/320], Loss: 1.7513\n",
      "Epoch [4/15], Step [275/320], Loss: 1.6956\n",
      "Epoch [4/15], Step [280/320], Loss: 1.7498\n",
      "Epoch [4/15], Step [285/320], Loss: 1.6455\n",
      "Epoch [4/15], Step [290/320], Loss: 1.6495\n",
      "Epoch [4/15], Step [295/320], Loss: 1.6590\n",
      "Epoch [4/15], Step [300/320], Loss: 1.6948\n",
      "Epoch [4/15], Step [305/320], Loss: 1.6478\n",
      "Epoch [4/15], Step [310/320], Loss: 1.6382\n",
      "Epoch [4/15], Step [315/320], Loss: 1.7826\n",
      "Epoch [4/15], Step [320/320], Loss: 1.6294\n",
      "Epoch [4/15]train Loss: 1.8194\n",
      "Epoch [4/15]val Loss: 0.4121\n",
      "Epoch 4/14\n",
      "----------\n",
      "Epoch [5/15], Step [5/320], Loss: 1.6222\n",
      "Epoch [5/15], Step [10/320], Loss: 1.6089\n",
      "Epoch [5/15], Step [15/320], Loss: 1.6071\n",
      "Epoch [5/15], Step [20/320], Loss: 1.7758\n",
      "Epoch [5/15], Step [25/320], Loss: 1.5796\n",
      "Epoch [5/15], Step [30/320], Loss: 1.6331\n",
      "Epoch [5/15], Step [35/320], Loss: 1.6177\n",
      "Epoch [5/15], Step [40/320], Loss: 1.6551\n",
      "Epoch [5/15], Step [45/320], Loss: 1.5754\n",
      "Epoch [5/15], Step [50/320], Loss: 1.5285\n",
      "Epoch [5/15], Step [55/320], Loss: 1.5652\n",
      "Epoch [5/15], Step [60/320], Loss: 1.5601\n",
      "Epoch [5/15], Step [65/320], Loss: 1.6496\n",
      "Epoch [5/15], Step [70/320], Loss: 1.6792\n",
      "Epoch [5/15], Step [75/320], Loss: 1.4895\n",
      "Epoch [5/15], Step [80/320], Loss: 1.6357\n",
      "Epoch [5/15], Step [85/320], Loss: 1.4941\n",
      "Epoch [5/15], Step [90/320], Loss: 1.5048\n",
      "Epoch [5/15], Step [95/320], Loss: 1.6391\n",
      "Epoch [5/15], Step [100/320], Loss: 1.5793\n",
      "Epoch [5/15], Step [105/320], Loss: 1.4844\n",
      "Epoch [5/15], Step [110/320], Loss: 1.4853\n",
      "Epoch [5/15], Step [115/320], Loss: 1.5375\n",
      "Epoch [5/15], Step [120/320], Loss: 1.4789\n",
      "Epoch [5/15], Step [125/320], Loss: 1.4536\n",
      "Epoch [5/15], Step [130/320], Loss: 1.6491\n",
      "Epoch [5/15], Step [135/320], Loss: 1.4705\n",
      "Epoch [5/15], Step [140/320], Loss: 1.5274\n",
      "Epoch [5/15], Step [145/320], Loss: 1.4158\n",
      "Epoch [5/15], Step [150/320], Loss: 1.4580\n",
      "Epoch [5/15], Step [155/320], Loss: 1.5210\n",
      "Epoch [5/15], Step [160/320], Loss: 1.5619\n",
      "Epoch [5/15], Step [165/320], Loss: 1.5212\n",
      "Epoch [5/15], Step [170/320], Loss: 1.3984\n",
      "Epoch [5/15], Step [175/320], Loss: 1.4498\n",
      "Epoch [5/15], Step [180/320], Loss: 1.4156\n",
      "Epoch [5/15], Step [185/320], Loss: 1.5958\n",
      "Epoch [5/15], Step [190/320], Loss: 1.4133\n",
      "Epoch [5/15], Step [195/320], Loss: 1.6018\n",
      "Epoch [5/15], Step [200/320], Loss: 1.4345\n",
      "Epoch [5/15], Step [205/320], Loss: 1.5357\n",
      "Epoch [5/15], Step [210/320], Loss: 1.4031\n",
      "Epoch [5/15], Step [215/320], Loss: 1.3828\n",
      "Epoch [5/15], Step [220/320], Loss: 1.3921\n",
      "Epoch [5/15], Step [225/320], Loss: 1.4581\n",
      "Epoch [5/15], Step [230/320], Loss: 1.4310\n",
      "Epoch [5/15], Step [235/320], Loss: 1.3813\n",
      "Epoch [5/15], Step [240/320], Loss: 1.3711\n",
      "Epoch [5/15], Step [245/320], Loss: 1.4095\n",
      "Epoch [5/15], Step [250/320], Loss: 1.3960\n",
      "Epoch [5/15], Step [255/320], Loss: 1.5847\n",
      "Epoch [5/15], Step [260/320], Loss: 1.4217\n",
      "Epoch [5/15], Step [265/320], Loss: 1.3890\n",
      "Epoch [5/15], Step [270/320], Loss: 1.5417\n",
      "Epoch [5/15], Step [275/320], Loss: 1.3970\n",
      "Epoch [5/15], Step [280/320], Loss: 1.4863\n",
      "Epoch [5/15], Step [285/320], Loss: 1.3266\n",
      "Epoch [5/15], Step [290/320], Loss: 1.3300\n",
      "Epoch [5/15], Step [295/320], Loss: 1.3364\n",
      "Epoch [5/15], Step [300/320], Loss: 1.4176\n",
      "Epoch [5/15], Step [305/320], Loss: 1.3665\n",
      "Epoch [5/15], Step [310/320], Loss: 1.3046\n",
      "Epoch [5/15], Step [315/320], Loss: 1.5370\n",
      "Epoch [5/15], Step [320/320], Loss: 1.2960\n",
      "Epoch [5/15]train Loss: 1.4998\n",
      "Epoch [5/15]val Loss: 0.3392\n",
      "Epoch 5/14\n",
      "----------\n",
      "Epoch [6/15], Step [5/320], Loss: 1.3167\n",
      "Epoch [6/15], Step [10/320], Loss: 1.3001\n",
      "Epoch [6/15], Step [15/320], Loss: 1.2949\n",
      "Epoch [6/15], Step [20/320], Loss: 1.5914\n",
      "Epoch [6/15], Step [25/320], Loss: 1.2540\n",
      "Epoch [6/15], Step [30/320], Loss: 1.3371\n",
      "Epoch [6/15], Step [35/320], Loss: 1.3254\n",
      "Epoch [6/15], Step [40/320], Loss: 1.3892\n",
      "Epoch [6/15], Step [45/320], Loss: 1.2616\n",
      "Epoch [6/15], Step [50/320], Loss: 1.1954\n",
      "Epoch [6/15], Step [55/320], Loss: 1.2616\n",
      "Epoch [6/15], Step [60/320], Loss: 1.2589\n",
      "Epoch [6/15], Step [65/320], Loss: 1.4307\n",
      "Epoch [6/15], Step [70/320], Loss: 1.5440\n",
      "Epoch [6/15], Step [75/320], Loss: 1.1511\n",
      "Epoch [6/15], Step [80/320], Loss: 1.4326\n",
      "Epoch [6/15], Step [85/320], Loss: 1.1683\n",
      "Epoch [6/15], Step [90/320], Loss: 1.1956\n",
      "Epoch [6/15], Step [95/320], Loss: 1.4264\n",
      "Epoch [6/15], Step [100/320], Loss: 1.3345\n",
      "Epoch [6/15], Step [105/320], Loss: 1.1890\n",
      "Epoch [6/15], Step [110/320], Loss: 1.1772\n",
      "Epoch [6/15], Step [115/320], Loss: 1.2976\n",
      "Epoch [6/15], Step [120/320], Loss: 1.1801\n",
      "Epoch [6/15], Step [125/320], Loss: 1.1467\n",
      "Epoch [6/15], Step [130/320], Loss: 1.4214\n",
      "Epoch [6/15], Step [135/320], Loss: 1.1695\n",
      "Epoch [6/15], Step [140/320], Loss: 1.2642\n",
      "Epoch [6/15], Step [145/320], Loss: 1.1009\n",
      "Epoch [6/15], Step [150/320], Loss: 1.1711\n",
      "Epoch [6/15], Step [155/320], Loss: 1.2686\n",
      "Epoch [6/15], Step [160/320], Loss: 1.3353\n",
      "Epoch [6/15], Step [165/320], Loss: 1.2931\n",
      "Epoch [6/15], Step [170/320], Loss: 1.0974\n",
      "Epoch [6/15], Step [175/320], Loss: 1.1828\n",
      "Epoch [6/15], Step [180/320], Loss: 1.1269\n",
      "Epoch [6/15], Step [185/320], Loss: 1.3846\n",
      "Epoch [6/15], Step [190/320], Loss: 1.1307\n",
      "Epoch [6/15], Step [195/320], Loss: 1.4359\n",
      "Epoch [6/15], Step [200/320], Loss: 1.1848\n",
      "Epoch [6/15], Step [205/320], Loss: 1.3227\n",
      "Epoch [6/15], Step [210/320], Loss: 1.1354\n",
      "Epoch [6/15], Step [215/320], Loss: 1.1030\n",
      "Epoch [6/15], Step [220/320], Loss: 1.1184\n",
      "Epoch [6/15], Step [225/320], Loss: 1.2408\n",
      "Epoch [6/15], Step [230/320], Loss: 1.1945\n",
      "Epoch [6/15], Step [235/320], Loss: 1.1171\n",
      "Epoch [6/15], Step [240/320], Loss: 1.0979\n",
      "Epoch [6/15], Step [245/320], Loss: 1.1654\n",
      "Epoch [6/15], Step [250/320], Loss: 1.1367\n",
      "Epoch [6/15], Step [255/320], Loss: 1.4584\n",
      "Epoch [6/15], Step [260/320], Loss: 1.1841\n",
      "Epoch [6/15], Step [265/320], Loss: 1.1424\n",
      "Epoch [6/15], Step [270/320], Loss: 1.3775\n",
      "Epoch [6/15], Step [275/320], Loss: 1.1620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/15], Step [280/320], Loss: 1.2805\n",
      "Epoch [6/15], Step [285/320], Loss: 1.0767\n",
      "Epoch [6/15], Step [290/320], Loss: 1.0808\n",
      "Epoch [6/15], Step [295/320], Loss: 1.0842\n",
      "Epoch [6/15], Step [300/320], Loss: 1.2008\n",
      "Epoch [6/15], Step [305/320], Loss: 1.1462\n",
      "Epoch [6/15], Step [310/320], Loss: 1.0442\n",
      "Epoch [6/15], Step [315/320], Loss: 1.3442\n",
      "Epoch [6/15], Step [320/320], Loss: 1.0361\n",
      "Epoch [6/15]train Loss: 1.2431\n",
      "Epoch [6/15]val Loss: 0.2825\n",
      "Epoch 6/14\n",
      "----------\n",
      "Epoch [7/15], Step [5/320], Loss: 1.0791\n",
      "Epoch [7/15], Step [10/320], Loss: 1.0614\n",
      "Epoch [7/15], Step [15/320], Loss: 1.0520\n",
      "Epoch [7/15], Step [20/320], Loss: 1.4460\n",
      "Epoch [7/15], Step [25/320], Loss: 1.0019\n",
      "Epoch [7/15], Step [30/320], Loss: 1.1090\n",
      "Epoch [7/15], Step [35/320], Loss: 1.0990\n",
      "Epoch [7/15], Step [40/320], Loss: 1.1837\n",
      "Epoch [7/15], Step [45/320], Loss: 1.0196\n",
      "Epoch [7/15], Step [50/320], Loss: 0.9385\n",
      "Epoch [7/15], Step [55/320], Loss: 1.0274\n",
      "Epoch [7/15], Step [60/320], Loss: 1.0271\n",
      "Epoch [7/15], Step [65/320], Loss: 1.2622\n",
      "Epoch [7/15], Step [70/320], Loss: 1.4381\n",
      "Epoch [7/15], Step [75/320], Loss: 0.8926\n",
      "Epoch [7/15], Step [80/320], Loss: 1.2765\n",
      "Epoch [7/15], Step [85/320], Loss: 0.9174\n",
      "Epoch [7/15], Step [90/320], Loss: 0.9589\n",
      "Epoch [7/15], Step [95/320], Loss: 1.2620\n",
      "Epoch [7/15], Step [100/320], Loss: 1.1460\n",
      "Epoch [7/15], Step [105/320], Loss: 0.9634\n",
      "Epoch [7/15], Step [110/320], Loss: 0.9421\n",
      "Epoch [7/15], Step [115/320], Loss: 1.1149\n",
      "Epoch [7/15], Step [120/320], Loss: 0.9526\n",
      "Epoch [7/15], Step [125/320], Loss: 0.9132\n",
      "Epoch [7/15], Step [130/320], Loss: 1.2473\n",
      "Epoch [7/15], Step [135/320], Loss: 0.9409\n",
      "Epoch [7/15], Step [140/320], Loss: 1.0639\n",
      "Epoch [7/15], Step [145/320], Loss: 0.8632\n",
      "Epoch [7/15], Step [150/320], Loss: 0.9536\n",
      "Epoch [7/15], Step [155/320], Loss: 1.0765\n",
      "Epoch [7/15], Step [160/320], Loss: 1.1611\n",
      "Epoch [7/15], Step [165/320], Loss: 1.1216\n",
      "Epoch [7/15], Step [170/320], Loss: 0.8697\n",
      "Epoch [7/15], Step [175/320], Loss: 0.9804\n",
      "Epoch [7/15], Step [180/320], Loss: 0.9090\n",
      "Epoch [7/15], Step [185/320], Loss: 1.2233\n",
      "Epoch [7/15], Step [190/320], Loss: 0.9181\n",
      "Epoch [7/15], Step [195/320], Loss: 1.3094\n",
      "Epoch [7/15], Step [200/320], Loss: 0.9964\n",
      "Epoch [7/15], Step [205/320], Loss: 1.1616\n",
      "Epoch [7/15], Step [210/320], Loss: 0.9342\n",
      "Epoch [7/15], Step [215/320], Loss: 0.8927\n",
      "Epoch [7/15], Step [220/320], Loss: 0.9125\n",
      "Epoch [7/15], Step [225/320], Loss: 1.0776\n",
      "Epoch [7/15], Step [230/320], Loss: 1.0167\n",
      "Epoch [7/15], Step [235/320], Loss: 0.9203\n",
      "Epoch [7/15], Step [240/320], Loss: 0.8938\n",
      "Epoch [7/15], Step [245/320], Loss: 0.9828\n",
      "Epoch [7/15], Step [250/320], Loss: 0.9426\n",
      "Epoch [7/15], Step [255/320], Loss: 1.3621\n",
      "Epoch [7/15], Step [260/320], Loss: 1.0065\n",
      "Epoch [7/15], Step [265/320], Loss: 0.9584\n",
      "Epoch [7/15], Step [270/320], Loss: 1.2549\n",
      "Epoch [7/15], Step [275/320], Loss: 0.9869\n",
      "Epoch [7/15], Step [280/320], Loss: 1.1273\n",
      "Epoch [7/15], Step [285/320], Loss: 0.8913\n",
      "Epoch [7/15], Step [290/320], Loss: 0.8962\n",
      "Epoch [7/15], Step [295/320], Loss: 0.8963\n",
      "Epoch [7/15], Step [300/320], Loss: 1.0397\n",
      "Epoch [7/15], Step [305/320], Loss: 0.9819\n",
      "Epoch [7/15], Step [310/320], Loss: 0.8519\n",
      "Epoch [7/15], Step [315/320], Loss: 1.2005\n",
      "Epoch [7/15], Step [320/320], Loss: 0.8441\n",
      "Epoch [7/15]train Loss: 1.0481\n",
      "Epoch [7/15]val Loss: 0.2406\n",
      "Epoch 7/14\n",
      "----------\n",
      "Epoch [8/15], Step [5/320], Loss: 0.9036\n",
      "Epoch [8/15], Step [10/320], Loss: 0.8851\n",
      "Epoch [8/15], Step [15/320], Loss: 0.8732\n",
      "Epoch [8/15], Step [20/320], Loss: 1.3370\n",
      "Epoch [8/15], Step [25/320], Loss: 0.8162\n",
      "Epoch [8/15], Step [30/320], Loss: 0.9408\n",
      "Epoch [8/15], Step [35/320], Loss: 0.9321\n",
      "Epoch [8/15], Step [40/320], Loss: 1.0320\n",
      "Epoch [8/15], Step [45/320], Loss: 0.8413\n",
      "Epoch [8/15], Step [50/320], Loss: 0.7493\n",
      "Epoch [8/15], Step [55/320], Loss: 0.8557\n",
      "Epoch [8/15], Step [60/320], Loss: 0.8575\n",
      "Epoch [8/15], Step [65/320], Loss: 1.1376\n",
      "Epoch [8/15], Step [70/320], Loss: 1.3561\n",
      "Epoch [8/15], Step [75/320], Loss: 0.7038\n",
      "Epoch [8/15], Step [80/320], Loss: 1.1597\n",
      "Epoch [8/15], Step [85/320], Loss: 0.7348\n",
      "Epoch [8/15], Step [90/320], Loss: 0.7861\n",
      "Epoch [8/15], Step [95/320], Loss: 1.1400\n",
      "Epoch [8/15], Step [100/320], Loss: 1.0070\n",
      "Epoch [8/15], Step [105/320], Loss: 0.7981\n",
      "Epoch [8/15], Step [110/320], Loss: 0.7705\n",
      "Epoch [8/15], Step [115/320], Loss: 0.9813\n",
      "Epoch [8/15], Step [120/320], Loss: 0.7868\n",
      "Epoch [8/15], Step [125/320], Loss: 0.7433\n",
      "Epoch [8/15], Step [130/320], Loss: 1.1189\n",
      "Epoch [8/15], Step [135/320], Loss: 0.7748\n",
      "Epoch [8/15], Step [140/320], Loss: 0.9183\n",
      "Epoch [8/15], Step [145/320], Loss: 0.6899\n",
      "Epoch [8/15], Step [150/320], Loss: 0.7970\n",
      "Epoch [8/15], Step [155/320], Loss: 0.9366\n",
      "Epoch [8/15], Step [160/320], Loss: 1.0334\n",
      "Epoch [8/15], Step [165/320], Loss: 0.9963\n",
      "Epoch [8/15], Step [170/320], Loss: 0.7048\n",
      "Epoch [8/15], Step [175/320], Loss: 0.8341\n",
      "Epoch [8/15], Step [180/320], Loss: 0.7510\n",
      "Epoch [8/15], Step [185/320], Loss: 1.1044\n",
      "Epoch [8/15], Step [190/320], Loss: 0.7643\n",
      "Epoch [8/15], Step [195/320], Loss: 1.2121\n",
      "Epoch [8/15], Step [200/320], Loss: 0.8597\n",
      "Epoch [8/15], Step [205/320], Loss: 1.0443\n",
      "Epoch [8/15], Step [210/320], Loss: 0.7891\n",
      "Epoch [8/15], Step [215/320], Loss: 0.7411\n",
      "Epoch [8/15], Step [220/320], Loss: 0.7641\n",
      "Epoch [8/15], Step [225/320], Loss: 0.9588\n",
      "Epoch [8/15], Step [230/320], Loss: 0.8881\n",
      "Epoch [8/15], Step [235/320], Loss: 0.7782\n",
      "Epoch [8/15], Step [240/320], Loss: 0.7470\n",
      "Epoch [8/15], Step [245/320], Loss: 0.8508\n",
      "Epoch [8/15], Step [250/320], Loss: 0.8023\n",
      "Epoch [8/15], Step [255/320], Loss: 1.2910\n",
      "Epoch [8/15], Step [260/320], Loss: 0.8787\n",
      "Epoch [8/15], Step [265/320], Loss: 0.8259\n",
      "Epoch [8/15], Step [270/320], Loss: 1.1659\n",
      "Epoch [8/15], Step [275/320], Loss: 0.8606\n",
      "Epoch [8/15], Step [280/320], Loss: 1.0171\n",
      "Epoch [8/15], Step [285/320], Loss: 0.7583\n",
      "Epoch [8/15], Step [290/320], Loss: 0.7636\n",
      "Epoch [8/15], Step [295/320], Loss: 0.7611\n",
      "Epoch [8/15], Step [300/320], Loss: 0.9238\n",
      "Epoch [8/15], Step [305/320], Loss: 0.8628\n",
      "Epoch [8/15], Step [310/320], Loss: 0.7142\n",
      "Epoch [8/15], Step [315/320], Loss: 1.0957\n",
      "Epoch [8/15], Step [320/320], Loss: 0.7067\n",
      "Epoch [8/15]train Loss: 0.9059\n",
      "Epoch [8/15]val Loss: 0.2105\n",
      "Epoch 8/14\n",
      "----------\n",
      "Epoch [9/15], Step [5/320], Loss: 0.7774\n",
      "Epoch [9/15], Step [10/320], Loss: 0.7583\n",
      "Epoch [9/15], Step [15/320], Loss: 0.7449\n",
      "Epoch [9/15], Step [20/320], Loss: 1.2557\n",
      "Epoch [9/15], Step [25/320], Loss: 0.6834\n",
      "Epoch [9/15], Step [30/320], Loss: 0.8202\n",
      "Epoch [9/15], Step [35/320], Loss: 0.8117\n",
      "Epoch [9/15], Step [40/320], Loss: 0.9224\n",
      "Epoch [9/15], Step [45/320], Loss: 0.7135\n",
      "Epoch [9/15], Step [50/320], Loss: 0.6144\n",
      "Epoch [9/15], Step [55/320], Loss: 0.7331\n",
      "Epoch [9/15], Step [60/320], Loss: 0.7364\n",
      "Epoch [9/15], Step [65/320], Loss: 1.0472\n",
      "Epoch [9/15], Step [70/320], Loss: 1.2939\n",
      "Epoch [9/15], Step [75/320], Loss: 0.5692\n",
      "Epoch [9/15], Step [80/320], Loss: 1.0738\n",
      "Epoch [9/15], Step [85/320], Loss: 0.6042\n",
      "Epoch [9/15], Step [90/320], Loss: 0.6627\n",
      "Epoch [9/15], Step [95/320], Loss: 1.0505\n",
      "Epoch [9/15], Step [100/320], Loss: 0.9069\n",
      "Epoch [9/15], Step [105/320], Loss: 0.6805\n",
      "Epoch [9/15], Step [110/320], Loss: 0.6485\n",
      "Epoch [9/15], Step [115/320], Loss: 0.8850\n",
      "Epoch [9/15], Step [120/320], Loss: 0.6683\n",
      "Epoch [9/15], Step [125/320], Loss: 0.6221\n",
      "Epoch [9/15], Step [130/320], Loss: 1.0262\n",
      "Epoch [9/15], Step [135/320], Loss: 0.6565\n",
      "Epoch [9/15], Step [140/320], Loss: 0.8142\n",
      "Epoch [9/15], Step [145/320], Loss: 0.5668\n",
      "Epoch [9/15], Step [150/320], Loss: 0.6840\n",
      "Epoch [9/15], Step [155/320], Loss: 0.8369\n",
      "Epoch [9/15], Step [160/320], Loss: 0.9420\n",
      "Epoch [9/15], Step [165/320], Loss: 0.9059\n",
      "Epoch [9/15], Step [170/320], Loss: 0.5873\n",
      "Epoch [9/15], Step [175/320], Loss: 0.7294\n",
      "Epoch [9/15], Step [180/320], Loss: 0.6385\n",
      "Epoch [9/15], Step [185/320], Loss: 1.0185\n",
      "Epoch [9/15], Step [190/320], Loss: 0.6545\n",
      "Epoch [9/15], Step [195/320], Loss: 1.1383\n",
      "Epoch [9/15], Step [200/320], Loss: 0.7621\n",
      "Epoch [9/15], Step [205/320], Loss: 0.9592\n",
      "Epoch [9/15], Step [210/320], Loss: 0.6859\n",
      "Epoch [9/15], Step [215/320], Loss: 0.6331\n",
      "Epoch [9/15], Step [220/320], Loss: 0.6583\n",
      "Epoch [9/15], Step [225/320], Loss: 0.8737\n",
      "Epoch [9/15], Step [230/320], Loss: 0.7961\n",
      "Epoch [9/15], Step [235/320], Loss: 0.6768\n",
      "Epoch [9/15], Step [240/320], Loss: 0.6428\n",
      "Epoch [9/15], Step [245/320], Loss: 0.7561\n",
      "Epoch [9/15], Step [250/320], Loss: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/15], Step [255/320], Loss: 1.2377\n",
      "Epoch [9/15], Step [260/320], Loss: 0.7875\n",
      "Epoch [9/15], Step [265/320], Loss: 0.7317\n",
      "Epoch [9/15], Step [270/320], Loss: 1.1005\n",
      "Epoch [9/15], Step [275/320], Loss: 0.7702\n",
      "Epoch [9/15], Step [280/320], Loss: 0.9383\n",
      "Epoch [9/15], Step [285/320], Loss: 0.6639\n",
      "Epoch [9/15], Step [290/320], Loss: 0.6692\n",
      "Epoch [9/15], Step [295/320], Loss: 0.6650\n",
      "Epoch [9/15], Step [300/320], Loss: 0.8406\n",
      "Epoch [9/15], Step [305/320], Loss: 0.7769\n",
      "Epoch [9/15], Step [310/320], Loss: 0.6164\n",
      "Epoch [9/15], Step [315/320], Loss: 1.0196\n",
      "Epoch [9/15], Step [320/320], Loss: 0.6088\n",
      "Epoch [9/15]train Loss: 0.8038\n",
      "Epoch [9/15]val Loss: 0.1890\n",
      "Epoch 9/14\n",
      "----------\n",
      "Epoch [10/15], Step [5/320], Loss: 0.6873\n",
      "Epoch [10/15], Step [10/320], Loss: 0.6681\n",
      "Epoch [10/15], Step [15/320], Loss: 0.6535\n",
      "Epoch [10/15], Step [20/320], Loss: 1.1946\n",
      "Epoch [10/15], Step [25/320], Loss: 0.5889\n",
      "Epoch [10/15], Step [30/320], Loss: 0.7342\n",
      "Epoch [10/15], Step [35/320], Loss: 0.7259\n",
      "Epoch [10/15], Step [40/320], Loss: 0.8433\n",
      "Epoch [10/15], Step [45/320], Loss: 0.6228\n",
      "Epoch [10/15], Step [50/320], Loss: 0.5185\n",
      "Epoch [10/15], Step [55/320], Loss: 0.6460\n",
      "Epoch [10/15], Step [60/320], Loss: 0.6502\n",
      "Epoch [10/15], Step [65/320], Loss: 0.9819\n",
      "Epoch [10/15], Step [70/320], Loss: 1.2461\n",
      "Epoch [10/15], Step [75/320], Loss: 0.4736\n",
      "Epoch [10/15], Step [80/320], Loss: 1.0106\n",
      "Epoch [10/15], Step [85/320], Loss: 0.5123\n",
      "Epoch [10/15], Step [90/320], Loss: 0.5748\n",
      "Epoch [10/15], Step [95/320], Loss: 0.9842\n",
      "Epoch [10/15], Step [100/320], Loss: 0.8342\n",
      "Epoch [10/15], Step [105/320], Loss: 0.5967\n",
      "Epoch [10/15], Step [110/320], Loss: 0.5614\n",
      "Epoch [10/15], Step [115/320], Loss: 0.8153\n",
      "Epoch [10/15], Step [120/320], Loss: 0.5839\n",
      "Epoch [10/15], Step [125/320], Loss: 0.5356\n",
      "Epoch [10/15], Step [130/320], Loss: 0.9590\n",
      "Epoch [10/15], Step [135/320], Loss: 0.5724\n",
      "Epoch [10/15], Step [140/320], Loss: 0.7391\n",
      "Epoch [10/15], Step [145/320], Loss: 0.4795\n",
      "Epoch [10/15], Step [150/320], Loss: 0.6041\n",
      "Epoch [10/15], Step [155/320], Loss: 0.7649\n",
      "Epoch [10/15], Step [160/320], Loss: 0.8759\n",
      "Epoch [10/15], Step [165/320], Loss: 0.8407\n",
      "Epoch [10/15], Step [170/320], Loss: 0.5038\n",
      "Epoch [10/15], Step [175/320], Loss: 0.6545\n",
      "Epoch [10/15], Step [180/320], Loss: 0.5582\n",
      "Epoch [10/15], Step [185/320], Loss: 0.9556\n",
      "Epoch [10/15], Step [190/320], Loss: 0.5757\n",
      "Epoch [10/15], Step [195/320], Loss: 1.0828\n",
      "Epoch [10/15], Step [200/320], Loss: 0.6915\n",
      "Epoch [10/15], Step [205/320], Loss: 0.8971\n",
      "Epoch [10/15], Step [210/320], Loss: 0.6123\n",
      "Epoch [10/15], Step [215/320], Loss: 0.5556\n",
      "Epoch [10/15], Step [220/320], Loss: 0.5827\n",
      "Epoch [10/15], Step [225/320], Loss: 0.8123\n",
      "Epoch [10/15], Step [230/320], Loss: 0.7296\n",
      "Epoch [10/15], Step [235/320], Loss: 0.6043\n",
      "Epoch [10/15], Step [240/320], Loss: 0.5683\n",
      "Epoch [10/15], Step [245/320], Loss: 0.6884\n",
      "Epoch [10/15], Step [250/320], Loss: 0.6299\n",
      "Epoch [10/15], Step [255/320], Loss: 1.1978\n",
      "Epoch [10/15], Step [260/320], Loss: 0.7215\n",
      "Epoch [10/15], Step [265/320], Loss: 0.6635\n",
      "Epoch [10/15], Step [270/320], Loss: 1.0520\n",
      "Epoch [10/15], Step [275/320], Loss: 0.7050\n",
      "Epoch [10/15], Step [280/320], Loss: 0.8814\n",
      "Epoch [10/15], Step [285/320], Loss: 0.5961\n",
      "Epoch [10/15], Step [290/320], Loss: 0.6013\n",
      "Epoch [10/15], Step [295/320], Loss: 0.5959\n",
      "Epoch [10/15], Step [300/320], Loss: 0.7801\n",
      "Epoch [10/15], Step [305/320], Loss: 0.7145\n",
      "Epoch [10/15], Step [310/320], Loss: 0.5464\n",
      "Epoch [10/15], Step [315/320], Loss: 0.9635\n",
      "Epoch [10/15], Step [320/320], Loss: 0.5383\n",
      "Epoch [10/15]train Loss: 0.7304\n",
      "Epoch [10/15]val Loss: 0.1735\n",
      "Epoch 10/14\n",
      "----------\n",
      "Epoch [11/15], Step [5/320], Loss: 0.6222\n",
      "Epoch [11/15], Step [10/320], Loss: 0.6031\n",
      "Epoch [11/15], Step [15/320], Loss: 0.5877\n",
      "Epoch [11/15], Step [20/320], Loss: 1.1476\n",
      "Epoch [11/15], Step [25/320], Loss: 0.5213\n",
      "Epoch [11/15], Step [30/320], Loss: 0.6720\n",
      "Epoch [11/15], Step [35/320], Loss: 0.6638\n",
      "Epoch [11/15], Step [40/320], Loss: 0.7855\n",
      "Epoch [11/15], Step [45/320], Loss: 0.5577\n",
      "Epoch [11/15], Step [50/320], Loss: 0.4500\n",
      "Epoch [11/15], Step [55/320], Loss: 0.5833\n",
      "Epoch [11/15], Step [60/320], Loss: 0.5879\n",
      "Epoch [11/15], Step [65/320], Loss: 0.9337\n",
      "Epoch [11/15], Step [70/320], Loss: 1.2087\n",
      "Epoch [11/15], Step [75/320], Loss: 0.4039\n",
      "Epoch [11/15], Step [80/320], Loss: 0.9637\n",
      "Epoch [11/15], Step [85/320], Loss: 0.4454\n",
      "Epoch [11/15], Step [90/320], Loss: 0.5114\n",
      "Epoch [11/15], Step [95/320], Loss: 0.9335\n",
      "Epoch [11/15], Step [100/320], Loss: 0.7806\n",
      "Epoch [11/15], Step [105/320], Loss: 0.5365\n",
      "Epoch [11/15], Step [110/320], Loss: 0.4990\n",
      "Epoch [11/15], Step [115/320], Loss: 0.7642\n",
      "Epoch [11/15], Step [120/320], Loss: 0.5230\n",
      "Epoch [11/15], Step [125/320], Loss: 0.4733\n",
      "Epoch [11/15], Step [130/320], Loss: 0.9096\n",
      "Epoch [11/15], Step [135/320], Loss: 0.5120\n",
      "Epoch [11/15], Step [140/320], Loss: 0.6850\n",
      "Epoch [11/15], Step [145/320], Loss: 0.4169\n",
      "Epoch [11/15], Step [150/320], Loss: 0.5462\n",
      "Epoch [11/15], Step [155/320], Loss: 0.7126\n",
      "Epoch [11/15], Step [160/320], Loss: 0.8270\n",
      "Epoch [11/15], Step [165/320], Loss: 0.7928\n",
      "Epoch [11/15], Step [170/320], Loss: 0.4431\n",
      "Epoch [11/15], Step [175/320], Loss: 0.5998\n",
      "Epoch [11/15], Step [180/320], Loss: 0.5005\n",
      "Epoch [11/15], Step [185/320], Loss: 0.9093\n",
      "Epoch [11/15], Step [190/320], Loss: 0.5186\n",
      "Epoch [11/15], Step [195/320], Loss: 1.0397\n",
      "Epoch [11/15], Step [200/320], Loss: 0.6394\n",
      "Epoch [11/15], Step [205/320], Loss: 0.8513\n",
      "Epoch [11/15], Step [210/320], Loss: 0.5590\n",
      "Epoch [11/15], Step [215/320], Loss: 0.4993\n",
      "Epoch [11/15], Step [220/320], Loss: 0.5278\n",
      "Epoch [11/15], Step [225/320], Loss: 0.7674\n",
      "Epoch [11/15], Step [230/320], Loss: 0.6810\n",
      "Epoch [11/15], Step [235/320], Loss: 0.5514\n",
      "Epoch [11/15], Step [240/320], Loss: 0.5144\n",
      "Epoch [11/15], Step [245/320], Loss: 0.6387\n",
      "Epoch [11/15], Step [250/320], Loss: 0.5771\n",
      "Epoch [11/15], Step [255/320], Loss: 1.1657\n",
      "Epoch [11/15], Step [260/320], Loss: 0.6734\n",
      "Epoch [11/15], Step [265/320], Loss: 0.6137\n",
      "Epoch [11/15], Step [270/320], Loss: 1.0155\n",
      "Epoch [11/15], Step [275/320], Loss: 0.6575\n",
      "Epoch [11/15], Step [280/320], Loss: 0.8393\n",
      "Epoch [11/15], Step [285/320], Loss: 0.5464\n",
      "Epoch [11/15], Step [290/320], Loss: 0.5516\n",
      "Epoch [11/15], Step [295/320], Loss: 0.5455\n",
      "Epoch [11/15], Step [300/320], Loss: 0.7353\n",
      "Epoch [11/15], Step [305/320], Loss: 0.6686\n",
      "Epoch [11/15], Step [310/320], Loss: 0.4955\n",
      "Epoch [11/15], Step [315/320], Loss: 0.9214\n",
      "Epoch [11/15], Step [320/320], Loss: 0.4869\n",
      "Epoch [11/15]train Loss: 0.6769\n",
      "Epoch [11/15]val Loss: 0.1622\n",
      "Epoch 11/14\n",
      "----------\n",
      "Epoch [12/15], Step [5/320], Loss: 0.5745\n",
      "Epoch [12/15], Step [10/320], Loss: 0.5565\n",
      "Epoch [12/15], Step [15/320], Loss: 0.5391\n",
      "Epoch [12/15], Step [20/320], Loss: 1.1102\n",
      "Epoch [12/15], Step [25/320], Loss: 0.4721\n",
      "Epoch [12/15], Step [30/320], Loss: 0.6264\n",
      "Epoch [12/15], Step [35/320], Loss: 0.6183\n",
      "Epoch [12/15], Step [40/320], Loss: 0.7428\n",
      "Epoch [12/15], Step [45/320], Loss: 0.5101\n",
      "Epoch [12/15], Step [50/320], Loss: 0.3996\n",
      "Epoch [12/15], Step [55/320], Loss: 0.5374\n",
      "Epoch [12/15], Step [60/320], Loss: 0.5423\n",
      "Epoch [12/15], Step [65/320], Loss: 0.8974\n",
      "Epoch [12/15], Step [70/320], Loss: 1.1788\n",
      "Epoch [12/15], Step [75/320], Loss: 0.3542\n",
      "Epoch [12/15], Step [80/320], Loss: 0.9278\n",
      "Epoch [12/15], Step [85/320], Loss: 0.3966\n",
      "Epoch [12/15], Step [90/320], Loss: 0.4651\n",
      "Epoch [12/15], Step [95/320], Loss: 0.8944\n",
      "Epoch [12/15], Step [100/320], Loss: 0.7405\n",
      "Epoch [12/15], Step [105/320], Loss: 0.4923\n",
      "Epoch [12/15], Step [110/320], Loss: 0.4528\n",
      "Epoch [12/15], Step [115/320], Loss: 0.7260\n",
      "Epoch [12/15], Step [120/320], Loss: 0.4786\n",
      "Epoch [12/15], Step [125/320], Loss: 0.4276\n",
      "Epoch [12/15], Step [130/320], Loss: 0.8723\n",
      "Epoch [12/15], Step [135/320], Loss: 0.4670\n",
      "Epoch [12/15], Step [140/320], Loss: 0.6446\n",
      "Epoch [12/15], Step [145/320], Loss: 0.3706\n",
      "Epoch [12/15], Step [150/320], Loss: 0.5038\n",
      "Epoch [12/15], Step [155/320], Loss: 0.6737\n",
      "Epoch [12/15], Step [160/320], Loss: 0.7901\n",
      "Epoch [12/15], Step [165/320], Loss: 0.7568\n",
      "Epoch [12/15], Step [170/320], Loss: 0.3988\n",
      "Epoch [12/15], Step [175/320], Loss: 0.5591\n",
      "Epoch [12/15], Step [180/320], Loss: 0.4581\n",
      "Epoch [12/15], Step [185/320], Loss: 0.8743\n",
      "Epoch [12/15], Step [190/320], Loss: 0.4760\n",
      "Epoch [12/15], Step [195/320], Loss: 1.0054\n",
      "Epoch [12/15], Step [200/320], Loss: 0.6006\n",
      "Epoch [12/15], Step [205/320], Loss: 0.8165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/15], Step [210/320], Loss: 0.5191\n",
      "Epoch [12/15], Step [215/320], Loss: 0.4575\n",
      "Epoch [12/15], Step [220/320], Loss: 0.4872\n",
      "Epoch [12/15], Step [225/320], Loss: 0.7335\n",
      "Epoch [12/15], Step [230/320], Loss: 0.6446\n",
      "Epoch [12/15], Step [235/320], Loss: 0.5123\n",
      "Epoch [12/15], Step [240/320], Loss: 0.4746\n",
      "Epoch [12/15], Step [245/320], Loss: 0.6015\n",
      "Epoch [12/15], Step [250/320], Loss: 0.5376\n",
      "Epoch [12/15], Step [255/320], Loss: 1.1392\n",
      "Epoch [12/15], Step [260/320], Loss: 0.6377\n",
      "Epoch [12/15], Step [265/320], Loss: 0.5765\n",
      "Epoch [12/15], Step [270/320], Loss: 0.9862\n",
      "Epoch [12/15], Step [275/320], Loss: 0.6219\n",
      "Epoch [12/15], Step [280/320], Loss: 0.8073\n",
      "Epoch [12/15], Step [285/320], Loss: 0.5094\n",
      "Epoch [12/15], Step [290/320], Loss: 0.5147\n",
      "Epoch [12/15], Step [295/320], Loss: 0.5080\n",
      "Epoch [12/15], Step [300/320], Loss: 0.7012\n",
      "Epoch [12/15], Step [305/320], Loss: 0.6342\n",
      "Epoch [12/15], Step [310/320], Loss: 0.4576\n",
      "Epoch [12/15], Step [315/320], Loss: 0.8888\n",
      "Epoch [12/15], Step [320/320], Loss: 0.4489\n",
      "Epoch [12/15]train Loss: 0.6369\n",
      "Epoch [12/15]val Loss: 0.1537\n",
      "Epoch 12/14\n",
      "----------\n",
      "Epoch [13/15], Step [5/320], Loss: 0.5388\n",
      "Epoch [13/15], Step [10/320], Loss: 0.5206\n",
      "Epoch [13/15], Step [15/320], Loss: 0.5030\n",
      "Epoch [13/15], Step [20/320], Loss: 1.0804\n",
      "Epoch [13/15], Step [25/320], Loss: 0.4355\n",
      "Epoch [13/15], Step [30/320], Loss: 0.5918\n",
      "Epoch [13/15], Step [35/320], Loss: 0.5843\n",
      "Epoch [13/15], Step [40/320], Loss: 0.7106\n",
      "Epoch [13/15], Step [45/320], Loss: 0.4745\n",
      "Epoch [13/15], Step [50/320], Loss: 0.3626\n",
      "Epoch [13/15], Step [55/320], Loss: 0.5033\n",
      "Epoch [13/15], Step [60/320], Loss: 0.5081\n",
      "Epoch [13/15], Step [65/320], Loss: 0.8694\n",
      "Epoch [13/15], Step [70/320], Loss: 1.1537\n",
      "Epoch [13/15], Step [75/320], Loss: 0.3168\n",
      "Epoch [13/15], Step [80/320], Loss: 0.8997\n",
      "Epoch [13/15], Step [85/320], Loss: 0.3608\n",
      "Epoch [13/15], Step [90/320], Loss: 0.4304\n",
      "Epoch [13/15], Step [95/320], Loss: 0.8630\n",
      "Epoch [13/15], Step [100/320], Loss: 0.7094\n",
      "Epoch [13/15], Step [105/320], Loss: 0.4590\n",
      "Epoch [13/15], Step [110/320], Loss: 0.4185\n",
      "Epoch [13/15], Step [115/320], Loss: 0.6965\n",
      "Epoch [13/15], Step [120/320], Loss: 0.4452\n",
      "Epoch [13/15], Step [125/320], Loss: 0.3934\n",
      "Epoch [13/15], Step [130/320], Loss: 0.8439\n",
      "Epoch [13/15], Step [135/320], Loss: 0.4335\n",
      "Epoch [13/15], Step [140/320], Loss: 0.6141\n",
      "Epoch [13/15], Step [145/320], Loss: 0.3363\n",
      "Epoch [13/15], Step [150/320], Loss: 0.4708\n",
      "Epoch [13/15], Step [155/320], Loss: 0.6445\n",
      "Epoch [13/15], Step [160/320], Loss: 0.7616\n",
      "Epoch [13/15], Step [165/320], Loss: 0.7293\n",
      "Epoch [13/15], Step [170/320], Loss: 0.3652\n",
      "Epoch [13/15], Step [175/320], Loss: 0.5283\n",
      "Epoch [13/15], Step [180/320], Loss: 0.4262\n",
      "Epoch [13/15], Step [185/320], Loss: 0.8475\n",
      "Epoch [13/15], Step [190/320], Loss: 0.4440\n",
      "Epoch [13/15], Step [195/320], Loss: 0.9774\n",
      "Epoch [13/15], Step [200/320], Loss: 0.5710\n",
      "Epoch [13/15], Step [205/320], Loss: 0.7889\n",
      "Epoch [13/15], Step [210/320], Loss: 0.4889\n",
      "Epoch [13/15], Step [215/320], Loss: 0.4260\n",
      "Epoch [13/15], Step [220/320], Loss: 0.4567\n",
      "Epoch [13/15], Step [225/320], Loss: 0.7071\n",
      "Epoch [13/15], Step [230/320], Loss: 0.6172\n",
      "Epoch [13/15], Step [235/320], Loss: 0.4827\n",
      "Epoch [13/15], Step [240/320], Loss: 0.4447\n",
      "Epoch [13/15], Step [245/320], Loss: 0.5732\n",
      "Epoch [13/15], Step [250/320], Loss: 0.5073\n",
      "Epoch [13/15], Step [255/320], Loss: 1.1161\n",
      "Epoch [13/15], Step [260/320], Loss: 0.6104\n",
      "Epoch [13/15], Step [265/320], Loss: 0.5483\n",
      "Epoch [13/15], Step [270/320], Loss: 0.9622\n",
      "Epoch [13/15], Step [275/320], Loss: 0.5945\n",
      "Epoch [13/15], Step [280/320], Loss: 0.7825\n",
      "Epoch [13/15], Step [285/320], Loss: 0.4812\n",
      "Epoch [13/15], Step [290/320], Loss: 0.4863\n",
      "Epoch [13/15], Step [295/320], Loss: 0.4793\n",
      "Epoch [13/15], Step [300/320], Loss: 0.6748\n",
      "Epoch [13/15], Step [305/320], Loss: 0.6075\n",
      "Epoch [13/15], Step [310/320], Loss: 0.4287\n",
      "Epoch [13/15], Step [315/320], Loss: 0.8631\n",
      "Epoch [13/15], Step [320/320], Loss: 0.4203\n",
      "Epoch [13/15]train Loss: 0.6066\n",
      "Epoch [13/15]val Loss: 0.1473\n",
      "Epoch 13/14\n",
      "----------\n",
      "Epoch [14/15], Step [5/320], Loss: 0.5117\n",
      "Epoch [14/15], Step [10/320], Loss: 0.4939\n",
      "Epoch [14/15], Step [15/320], Loss: 0.4752\n",
      "Epoch [14/15], Step [20/320], Loss: 1.0558\n",
      "Epoch [14/15], Step [25/320], Loss: 0.4079\n",
      "Epoch [14/15], Step [30/320], Loss: 0.5653\n",
      "Epoch [14/15], Step [35/320], Loss: 0.5582\n",
      "Epoch [14/15], Step [40/320], Loss: 0.6860\n",
      "Epoch [14/15], Step [45/320], Loss: 0.4477\n",
      "Epoch [14/15], Step [50/320], Loss: 0.3342\n",
      "Epoch [14/15], Step [55/320], Loss: 0.4773\n",
      "Epoch [14/15], Step [60/320], Loss: 0.4820\n",
      "Epoch [14/15], Step [65/320], Loss: 0.8473\n",
      "Epoch [14/15], Step [70/320], Loss: 1.1321\n",
      "Epoch [14/15], Step [75/320], Loss: 0.2898\n",
      "Epoch [14/15], Step [80/320], Loss: 0.8771\n",
      "Epoch [14/15], Step [85/320], Loss: 0.3332\n",
      "Epoch [14/15], Step [90/320], Loss: 0.4041\n",
      "Epoch [14/15], Step [95/320], Loss: 0.8373\n",
      "Epoch [14/15], Step [100/320], Loss: 0.6851\n",
      "Epoch [14/15], Step [105/320], Loss: 0.4334\n",
      "Epoch [14/15], Step [110/320], Loss: 0.3924\n",
      "Epoch [14/15], Step [115/320], Loss: 0.6736\n",
      "Epoch [14/15], Step [120/320], Loss: 0.4199\n",
      "Epoch [14/15], Step [125/320], Loss: 0.3672\n",
      "Epoch [14/15], Step [130/320], Loss: 0.8212\n",
      "Epoch [14/15], Step [135/320], Loss: 0.4074\n",
      "Epoch [14/15], Step [140/320], Loss: 0.5903\n",
      "Epoch [14/15], Step [145/320], Loss: 0.3105\n",
      "Epoch [14/15], Step [150/320], Loss: 0.4473\n",
      "Epoch [14/15], Step [155/320], Loss: 0.6218\n",
      "Epoch [14/15], Step [160/320], Loss: 0.7393\n",
      "Epoch [14/15], Step [165/320], Loss: 0.7081\n",
      "Epoch [14/15], Step [170/320], Loss: 0.3398\n",
      "Epoch [14/15], Step [175/320], Loss: 0.5047\n",
      "Epoch [14/15], Step [180/320], Loss: 0.4021\n",
      "Epoch [14/15], Step [185/320], Loss: 0.8264\n",
      "Epoch [14/15], Step [190/320], Loss: 0.4195\n",
      "Epoch [14/15], Step [195/320], Loss: 0.9535\n",
      "Epoch [14/15], Step [200/320], Loss: 0.5480\n",
      "Epoch [14/15], Step [205/320], Loss: 0.7666\n",
      "Epoch [14/15], Step [210/320], Loss: 0.4655\n",
      "Epoch [14/15], Step [215/320], Loss: 0.4016\n",
      "Epoch [14/15], Step [220/320], Loss: 0.4331\n",
      "Epoch [14/15], Step [225/320], Loss: 0.6863\n",
      "Epoch [14/15], Step [230/320], Loss: 0.5957\n",
      "Epoch [14/15], Step [235/320], Loss: 0.4596\n",
      "Epoch [14/15], Step [240/320], Loss: 0.4217\n",
      "Epoch [14/15], Step [245/320], Loss: 0.5508\n",
      "Epoch [14/15], Step [250/320], Loss: 0.4837\n",
      "Epoch [14/15], Step [255/320], Loss: 1.0965\n",
      "Epoch [14/15], Step [260/320], Loss: 0.5892\n",
      "Epoch [14/15], Step [265/320], Loss: 0.5265\n",
      "Epoch [14/15], Step [270/320], Loss: 0.9421\n",
      "Epoch [14/15], Step [275/320], Loss: 0.5730\n",
      "Epoch [14/15], Step [280/320], Loss: 0.7626\n",
      "Epoch [14/15], Step [285/320], Loss: 0.4593\n",
      "Epoch [14/15], Step [290/320], Loss: 0.4645\n",
      "Epoch [14/15], Step [295/320], Loss: 0.4571\n",
      "Epoch [14/15], Step [300/320], Loss: 0.6539\n",
      "Epoch [14/15], Step [305/320], Loss: 0.5864\n",
      "Epoch [14/15], Step [310/320], Loss: 0.4066\n",
      "Epoch [14/15], Step [315/320], Loss: 0.8423\n",
      "Epoch [14/15], Step [320/320], Loss: 0.3978\n",
      "Epoch [14/15]train Loss: 0.5829\n",
      "Epoch [14/15]val Loss: 0.1422\n",
      "Epoch 14/14\n",
      "----------\n",
      "Epoch [15/15], Step [5/320], Loss: 0.4904\n",
      "Epoch [15/15], Step [10/320], Loss: 0.4734\n",
      "Epoch [15/15], Step [15/320], Loss: 0.4536\n",
      "Epoch [15/15], Step [20/320], Loss: 1.0346\n",
      "Epoch [15/15], Step [25/320], Loss: 0.3866\n",
      "Epoch [15/15], Step [30/320], Loss: 0.5446\n",
      "Epoch [15/15], Step [35/320], Loss: 0.5381\n",
      "Epoch [15/15], Step [40/320], Loss: 0.6664\n",
      "Epoch [15/15], Step [45/320], Loss: 0.4272\n",
      "Epoch [15/15], Step [50/320], Loss: 0.3120\n",
      "Epoch [15/15], Step [55/320], Loss: 0.4574\n",
      "Epoch [15/15], Step [60/320], Loss: 0.4618\n",
      "Epoch [15/15], Step [65/320], Loss: 0.8293\n",
      "Epoch [15/15], Step [70/320], Loss: 1.1131\n",
      "Epoch [15/15], Step [75/320], Loss: 0.2682\n",
      "Epoch [15/15], Step [80/320], Loss: 0.8586\n",
      "Epoch [15/15], Step [85/320], Loss: 0.3117\n",
      "Epoch [15/15], Step [90/320], Loss: 0.3834\n",
      "Epoch [15/15], Step [95/320], Loss: 0.8165\n",
      "Epoch [15/15], Step [100/320], Loss: 0.6656\n",
      "Epoch [15/15], Step [105/320], Loss: 0.4135\n",
      "Epoch [15/15], Step [110/320], Loss: 0.3720\n",
      "Epoch [15/15], Step [115/320], Loss: 0.6553\n",
      "Epoch [15/15], Step [120/320], Loss: 0.3999\n",
      "Epoch [15/15], Step [125/320], Loss: 0.3466\n",
      "Epoch [15/15], Step [130/320], Loss: 0.8025\n",
      "Epoch [15/15], Step [135/320], Loss: 0.3871\n",
      "Epoch [15/15], Step [140/320], Loss: 0.5717\n",
      "Epoch [15/15], Step [145/320], Loss: 0.2899\n",
      "Epoch [15/15], Step [150/320], Loss: 0.4271\n",
      "Epoch [15/15], Step [155/320], Loss: 0.6041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Step [160/320], Loss: 0.7214\n",
      "Epoch [15/15], Step [165/320], Loss: 0.6914\n",
      "Epoch [15/15], Step [170/320], Loss: 0.3198\n",
      "Epoch [15/15], Step [175/320], Loss: 0.4859\n",
      "Epoch [15/15], Step [180/320], Loss: 0.3833\n",
      "Epoch [15/15], Step [185/320], Loss: 0.8095\n",
      "Epoch [15/15], Step [190/320], Loss: 0.4003\n",
      "Epoch [15/15], Step [195/320], Loss: 0.9343\n",
      "Epoch [15/15], Step [200/320], Loss: 0.5295\n",
      "Epoch [15/15], Step [205/320], Loss: 0.7488\n",
      "Epoch [15/15], Step [210/320], Loss: 0.4471\n",
      "Epoch [15/15], Step [215/320], Loss: 0.3824\n",
      "Epoch [15/15], Step [220/320], Loss: 0.4146\n",
      "Epoch [15/15], Step [225/320], Loss: 0.6697\n",
      "Epoch [15/15], Step [230/320], Loss: 0.5784\n",
      "Epoch [15/15], Step [235/320], Loss: 0.4413\n",
      "Epoch [15/15], Step [240/320], Loss: 0.4035\n",
      "Epoch [15/15], Step [245/320], Loss: 0.5332\n",
      "Epoch [15/15], Step [250/320], Loss: 0.4650\n",
      "Epoch [15/15], Step [255/320], Loss: 1.0796\n",
      "Epoch [15/15], Step [260/320], Loss: 0.5722\n",
      "Epoch [15/15], Step [265/320], Loss: 0.5092\n",
      "Epoch [15/15], Step [270/320], Loss: 0.9251\n",
      "Epoch [15/15], Step [275/320], Loss: 0.5560\n",
      "Epoch [15/15], Step [280/320], Loss: 0.7464\n",
      "Epoch [15/15], Step [285/320], Loss: 0.4419\n",
      "Epoch [15/15], Step [290/320], Loss: 0.4470\n",
      "Epoch [15/15], Step [295/320], Loss: 0.4395\n",
      "Epoch [15/15], Step [300/320], Loss: 0.6367\n",
      "Epoch [15/15], Step [305/320], Loss: 0.5694\n",
      "Epoch [15/15], Step [310/320], Loss: 0.3891\n",
      "Epoch [15/15], Step [315/320], Loss: 0.8251\n",
      "Epoch [15/15], Step [320/320], Loss: 0.3804\n",
      "Epoch [15/15]train Loss: 0.5642\n",
      "Epoch [15/15]val Loss: 0.1383\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 15\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range( num_epoch):\n",
    "    \n",
    "    print('Epoch {}/{}'.format(epoch, num_epoch- 1))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            languagemodel.train() # Set model to training mode\n",
    "        else:\n",
    "            languagemodel.eval() # Set model to evaluate mode\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        # Iterate over data.\n",
    "        \n",
    "        for i, (texts,labels) in enumerate(data_loaders[phase]):\n",
    "            \n",
    "            texts.sort(key=len, reverse=True)\n",
    "            labels.sort(key=len, reverse=True)\n",
    "            \n",
    "            texts_pad = pad(texts)\n",
    "            labels_pad = pad(labels)\n",
    "            # torch can only train on Variable, so convert them to Variable\n",
    "            texts_pad = Variable(torch.LongTensor(texts_pad))\n",
    "            labels_pad = Variable(torch.LongTensor(labels_pad))\n",
    "\n",
    "            outputs = languagemodel(texts_pad,get_lengths(texts)) # Forward pass: compute the output class given a image\n",
    "\n",
    "            #print outputs.view(-1,outputs.size(2)).size()\n",
    "            #print outputs.contiguous().view(-1,tokens_count).size()\n",
    "\n",
    "            #print labels_pad.view(-1).size()\n",
    "            #print torch.LongTensor(pad(labels).flatten()).size()\n",
    "\n",
    "            loss = criterion(outputs.view(-1,outputs.size(2)),labels_pad.view(-1)) # Compute the loss: difference between the output class and the pre-given label\n",
    "            #print loss\n",
    "\n",
    "            optimizer.zero_grad() # clear gradients for next train\n",
    "            if phase == 'train':\n",
    "\n",
    "                loss.backward() # backpropagation, compute gradients\n",
    "                optimizer.step() # apply gradients  and update the weights of hidden nodes\n",
    "                \n",
    "            running_loss += loss.data * texts_pad.size(0)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                if (i+1) % 5 == 0 :\n",
    "                    print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %(epoch+1, num_epoch, i+1, data_lengths[phase]//10, loss.data))\n",
    "                    \n",
    "        epoch_loss = running_loss / data_lengths[phase]\n",
    "        #print epoch_loss\n",
    "        if phase =='train':\n",
    "            train_losses.append(epoch_loss)\n",
    "        if phase =='val':\n",
    "            valid_losses.append(epoch_loss)\n",
    "            \n",
    "        print('Epoch [{}/{}]{} Loss: {:.4f}'.format(epoch+1, num_epoch,phase, epoch_loss))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 8, 1, 20, 32, 19, 27, 20, 8, 5, 27, 4, 9, 6, 6, 5, 18, 14, 3, 44]\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "d\n",
      "i\n",
      "f\n",
      "f\n",
      "e\n",
      "r\n",
      "e\n",
      "e\n",
      "e\n",
      " \n"
     ]
    }
   ],
   "source": [
    "enc = text_encoder(\"what's the differnc\")\n",
    "print enc\n",
    "enc_pad = pad(np.array([enc]))\n",
    "#print enc_pad\n",
    "tensor_pad = Variable(torch.LongTensor(enc_pad))\n",
    "#print tensor_pad\n",
    "out = languagemodel(tensor_pad,get_lengths(np.array([enc])))\n",
    "#int(output.cpu().data.topk(1,dim=2)[1].numpy()[0])\n",
    "for i in range(out.size()[1]):\n",
    "    t = out[:,i,:]\n",
    "    _,predicted = torch.max(t.data,1)\n",
    "    #print predicted\n",
    "    print id_word[predicted.numpy()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8],\n",
       "       [ 1],\n",
       "       [20],\n",
       "       [27],\n",
       "       [ 1]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.cpu().data.topk(1,dim=2)[1].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sequence(start_string, max_length):\n",
    "    enc = text_encoder(start_string)\n",
    "    enc_pad = pad(np.array([enc]))\n",
    "    tensor_pad = Variable(torch.LongTensor(enc_pad))\n",
    "    result = start_string\n",
    "    \n",
    "    while enc[0] != token_dict['<end>']:\n",
    "        out = languagemodel(tensor_pad,get_lengths(np.array([enc])))\n",
    "        enc = [int(out.cpu().data.topk(1,dim=2)[1].numpy()[0][-1])]\n",
    "        result+=id_word[enc[0]]\n",
    "        #print result\n",
    "        if len(result)>max_length:\n",
    "            break\n",
    "        enc = text_encoder(result)\n",
    "        enc_pad = pad(np.array([enc]))\n",
    "        tensor_pad = Variable(torch.LongTensor(enc_pad))\n",
    "    print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what's the difference between a jews epe eeee eeee \n"
     ]
    }
   ],
   "source": [
    "generate_sequence(\"what's the difference between a jew\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_encoded = []\n",
    "for char in \"start string\":\n",
    "    input_encoded.append(token_dict[char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 20, 1, 18, 20, 27, 19, 20, 18, 9, 14, 7]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 8, 1, 20, 44]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
